{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janewarren/ai_alt_text/blob/main/alt_text_analysis_pt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plots"
      ],
      "metadata": {
        "id": "So7k_mmJONEy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x6QkCV7Zvyg"
      },
      "source": [
        "##Chatgpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjVZ1CLYRwhb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sz1 = 12\n",
        "sz2 = 60"
      ],
      "metadata": {
        "id": "Vu8oANQxC5AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLrmpmMgoATT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/image_captioning_bias/chatgpt/captions.csv\"\n",
        "GENDER_MAP = {1: \"male\", 2: \"female\"}\n",
        "RACE_MAP = {1: \"skin1\", 2: \"skin2\", 3: \"skin3\", 4: \"skin4\", 5: \"skin5\", 6: \"skin6\"}\n",
        "WARMTH_LEXICON = {\"caring\",\"nurturing\",\"friendly\",\"warm\",\"kind\",\"gentle\",\"compassionate\",\"loving\",\"supportive\",\"affectionate\"}\n",
        "COMPETENCE_LEXICON = {\"skilled\",\"competent\",\"expert\",\"talented\",\"proficient\",\"capable\",\"strong\",\"efficient\",\"accomplished\"}\n",
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "def find_text_column(df):\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\",\"Description\"]\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "        lowcols = {col.lower(): col for col in df.columns}\n",
        "        if c.lower() in lowcols:\n",
        "            return lowcols[c.lower()]\n",
        "    string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "    if not string_cols:\n",
        "        raise KeyError(\"No string-like columns found\")\n",
        "    avg_len = {col: df[col].astype(str).map(len).mean() for col in string_cols}\n",
        "    return max(avg_len, key=avg_len.get)\n",
        "\n",
        "def extract_numbers(filename):\n",
        "    parts = str(filename).split('_')\n",
        "    if len(parts) >= 2:\n",
        "        try:\n",
        "            return int(parts[0]), int(parts[1])\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    return None, None\n",
        "\n",
        "def load_and_prepare(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    text_col = find_text_column(df)\n",
        "    df = df.copy()\n",
        "    df.rename(columns={text_col: \"caption\"}, inplace=True)\n",
        "    df[['first_num','second_num']] = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "    df['gender_label'] = df['first_num'].map(GENDER_MAP).fillna(\"unknown\")\n",
        "    df['race_label'] = df['second_num'].map(RACE_MAP).fillna(\"unknown\")\n",
        "    split_dataframes = {}\n",
        "    for first in [1,2]:\n",
        "        for second in range(1,7):\n",
        "            name = f\"df_{first}_{second}\"\n",
        "            sdf = df[(df['first_num']==first) & (df['second_num']==second)].copy().reset_index(drop=True)\n",
        "            if not sdf.empty:\n",
        "                sdf['group_label'] = f\"{sdf['gender_label'].iloc[0]}_{sdf['race_label'].iloc[0]}\"\n",
        "            else:\n",
        "                sdf['group_label'] = \"\"\n",
        "            split_dataframes[name] = sdf\n",
        "    return df, split_dataframes\n",
        "\n",
        "def compute_caption_embeddings(captions, batch_size=64):\n",
        "    embs = model.encode(list(captions), convert_to_numpy=True, show_progress_bar=False, batch_size=batch_size)\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    norms[norms==0] = 1.0\n",
        "    embs = embs / norms\n",
        "    return embs\n",
        "\n",
        "def compute_embedding_centroid(seeds):\n",
        "    if not seeds:\n",
        "        raise ValueError(\"Empty seeds\")\n",
        "    emb = model.encode(seeds, convert_to_numpy=True, show_progress_bar=False)\n",
        "    centroid = emb.mean(axis=0)\n",
        "    norm = np.linalg.norm(centroid)\n",
        "    if norm > 0:\n",
        "        centroid = centroid / norm\n",
        "    return centroid\n",
        "\n",
        "def cos_sim_to_centroid(caption_embs, centroid):\n",
        "    return np.dot(caption_embs, centroid)\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^\\w\\s']\", \" \", text)\n",
        "    tokens = [t for t in text.split() if t]\n",
        "    return tokens\n",
        "def lexicon_scores_for_caption(caption, warmth_lex=WARMTH_LEXICON, comp_lex=COMPETENCE_LEXICON):\n",
        "    tokens = simple_tokenize(caption)\n",
        "    token_count = len(tokens)\n",
        "    warmth_count = sum(1 for t in tokens if t in warmth_lex)\n",
        "    comp_count = sum(1 for t in tokens if t in comp_lex)\n",
        "    return {\"word_count\": token_count, \"warmth_count\": warmth_count, \"competence_count\": comp_count, \"warmth_rate\": warmth_count / token_count if token_count else 0.0, \"competence_rate\": comp_count / token_count if token_count else 0.0}\n",
        "def analyze_group_df(group_df, warmth_seeds=list(WARMTH_LEXICON), comp_seeds=list(COMPETENCE_LEXICON)):\n",
        "    if group_df is None or len(group_df)==0:\n",
        "        return pd.DataFrame(), None\n",
        "    lex_rows = [lexicon_scores_for_caption(c) for c in group_df['caption']]\n",
        "    lex_df = pd.DataFrame(lex_rows)\n",
        "    cap_embs = compute_caption_embeddings(group_df['caption'].tolist())\n",
        "    warmth_centroid = compute_embedding_centroid(warmth_seeds)\n",
        "    comp_centroid = compute_embedding_centroid(comp_seeds)\n",
        "    warmth_sim = cos_sim_to_centroid(cap_embs, warmth_centroid)\n",
        "    comp_sim = cos_sim_to_centroid(cap_embs, comp_centroid)\n",
        "    out = group_df.copy().reset_index(drop=True)\n",
        "    out = pd.concat([out, lex_df], axis=1)\n",
        "    out['warmth_emb_sim'] = warmth_sim\n",
        "    out['competence_emb_sim'] = comp_sim\n",
        "    return out, cap_embs\n",
        "if __name__ == \"__main__\":\n",
        "    base_df, split_dataframes = load_and_prepare(CSV_PATH)\n",
        "    analysis_results = {}\n",
        "    embeddings_store = {}\n",
        "    for name, sdf in split_dataframes.items():\n",
        "        if sdf is None or sdf.empty:\n",
        "            analysis_results[name] = pd.DataFrame()\n",
        "            embeddings_store[name] = None\n",
        "            continue\n",
        "        res_df, embs = analyze_group_df(sdf)\n",
        "        analysis_results[name] = res_df\n",
        "        embeddings_store[name] = embs\n",
        "\n",
        "    male_all = pd.concat([analysis_results.get(f\"df_1_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    female_all = pd.concat([analysis_results.get(f\"df_2_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    male_all = male_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "    female_all = female_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "    warmth_centroid = compute_embedding_centroid(list(WARMTH_LEXICON))\n",
        "    comp_centroid = compute_embedding_centroid(list(COMPETENCE_LEXICON))\n",
        "\n",
        "    outdir = \"/content/drive/MyDrive/image_captioning_bias/images/chatgpt\"\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    jitter = 0.0\n",
        "\n",
        "    x_m = male_all['warmth_emb_sim'].values + np.random.normal(0, jitter, size=male_all.shape[0])\n",
        "    y_m = male_all['competence_emb_sim'].values + np.random.normal(0, jitter, size=male_all.shape[0])\n",
        "    x_f = female_all['warmth_emb_sim'].values + np.random.normal(0, jitter, size=female_all.shape[0])\n",
        "    y_f = female_all['competence_emb_sim'].values + np.random.normal(0, jitter, size=female_all.shape[0])\n",
        "    plt.scatter(x_m, y_m, alpha=0.6, label='male', s=sz1)\n",
        "    plt.scatter(x_f, y_f, alpha=0.6, label='female', s=sz1)\n",
        "\n",
        "    if len(x_m) >= 2 and np.nanstd(x_m) > 0:\n",
        "        coeffs_m = np.polyfit(x_m, y_m, 1)\n",
        "        xs = np.linspace(np.nanmin(np.concatenate([x_m,x_f])), np.nanmax(np.concatenate([x_m,x_f])), 200)\n",
        "        ys_m = coeffs_m[0]*xs + coeffs_m[1]\n",
        "        plt.plot(xs, ys_m, color='black', linestyle='-', linewidth=2, label=f'male fit (slope={coeffs_m[0]:.4f})')\n",
        "\n",
        "    if len(x_f) >= 2 and np.nanstd(x_f) > 0:\n",
        "        coeffs_f = np.polyfit(x_f, y_f, 1)\n",
        "        xs = np.linspace(np.nanmin(np.concatenate([x_m,x_f])), np.nanmax(np.concatenate([x_m,x_f])), 200)\n",
        "        ys_f = coeffs_f[0]*xs + coeffs_f[1]\n",
        "        plt.plot(xs, ys_f, color='gray', linestyle='--', linewidth=2, label=f'female fit (slope={coeffs_f[0]:.4f})')\n",
        "    cx_m, cy_m = np.mean(x_m), np.mean(y_m)\n",
        "    cx_f, cy_f = np.mean(x_f), np.mean(y_f)\n",
        "    plt.scatter([cx_m],[cy_m], marker='X', s=60)\n",
        "    plt.text(cx_m, cy_m, \"  male centroid\", fontsize=9)\n",
        "    plt.scatter([cx_f],[cy_f], marker='D', s=60)\n",
        "    plt.text(cx_f, cy_f, \"  female centroid\", fontsize=9)\n",
        "    plt.xlabel(\"Warmth similarity (embedding centroid)\")\n",
        "    plt.ylabel(\"Competence similarity (embedding centroid)\")\n",
        "    plt.title(\"Warmth vs Competence — Male vs Female (ChatGPT)\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    outfig = os.path.join(outdir, \"chatgpt_agg_male_vs_female_warmth_competence_with_slopes.png\")\n",
        "    plt.savefig(outfig, dpi=300)\n",
        "    plt.show()\n",
        "    SKIN_GROUPS = {\"light\":[1,2], \"mid\":[3,4], \"dark\":[5,6]}\n",
        "    skin_agg_results = {}\n",
        "    skin_embs = {}\n",
        "\n",
        "    for s in range(1,7):\n",
        "        name_m = f\"df_1_{s}\"\n",
        "        name_f = f\"df_2_{s}\"\n",
        "        combined_raw = pd.concat([analysis_results.get(name_m, pd.DataFrame()), analysis_results.get(name_f, pd.DataFrame())], ignore_index=True)\n",
        "        if combined_raw is None or combined_raw.empty:\n",
        "            continue\n",
        "        skin_res = combined_raw.copy()\n",
        "        skin_res['skin'] = f\"skin{s}\"\n",
        "        skin_agg_results[s] = skin_res\n",
        "\n",
        "    group_results = {}\n",
        "    group_embs = {}\n",
        "\n",
        "    for group_name, skin_list in SKIN_GROUPS.items():\n",
        "        dfs = []\n",
        "        for s in skin_list:\n",
        "            sdf = skin_agg_results.get(s)\n",
        "            if sdf is not None and not sdf.empty:\n",
        "                dfs.append(sdf)\n",
        "        if not dfs:\n",
        "            continue\n",
        "        combined = pd.concat(dfs, ignore_index=True)\n",
        "        combined = combined.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "        group_results[group_name] = combined\n",
        "    group_colors = {\"light\":\"#1f77b4\", \"mid\":\"#ff7f0e\", \"dark\":\"#2ca02c\"}\n",
        "    group_markers = {\"light\":\"o\", \"mid\":\"^\", \"dark\":\"s\"}\n",
        "    plt.figure(figsize=(9,7))\n",
        "\n",
        "    slopes = {}\n",
        "\n",
        "    for group_name in group_results.keys():\n",
        "        gdf = group_results[group_name]\n",
        "        x = gdf['warmth_emb_sim'].values\n",
        "        y = gdf['competence_emb_sim'].values\n",
        "        plt.scatter(x, y, label=group_name, alpha=0.6, s=sz1, color=group_colors.get(group_name,None))\n",
        "        if len(x) >= 2 and np.nanstd(x) > 0:\n",
        "            coeffs = np.polyfit(x, y, 1)\n",
        "            xs_line = np.linspace(float(np.nanmin(x)), float(np.nanmax(x)), 200)\n",
        "            ys_line = coeffs[0]*xs_line + coeffs[1]\n",
        "            plt.plot(xs_line, ys_line, color=group_colors.get(group_name,None), linewidth=2, label=f\"{group_name} fit (slope={coeffs[0]:.4f})\")\n",
        "            slopes[group_name] = float(coeffs[0])\n",
        "        cx = np.mean(x); cy = np.mean(y)\n",
        "        plt.scatter([cx],[cy], s=60, color=group_colors.get(group_name,None))\n",
        "        plt.text(cx, cy, f\"  {group_name}\", fontsize=9)\n",
        "\n",
        "    plt.xlabel(\"Warmth similarity\")\n",
        "    plt.ylabel(\"Competence similarity\")\n",
        "    plt.title(\"Warmth vs Competence — Skin groups (light/mid/dark) (ChatGPT)\")\n",
        "    plt.legend(title=\"skin group\", loc='best')\n",
        "    plt.tight_layout()\n",
        "    out_comb_warm = os.path.join(outdir, \"chatgpt_combined_skingroups_warmth_competence_with_slopes.png\")\n",
        "    plt.savefig(out_comb_warm, dpi=300)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outfig)\n",
        "    print(\"Saved:\", out_comb_warm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FIcQh_YqUQ7"
      },
      "source": [
        "##Claude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OStA5NsSssbN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/image_captioning_bias/claude/captions.csv\"\n",
        "GENDER_MAP = {1: \"male\", 2: \"female\"}\n",
        "RACE_MAP = {1: \"skin1\", 2: \"skin2\", 3: \"skin3\", 4: \"skin4\", 5: \"skin5\", 6: \"skin6\"}\n",
        "WARMTH_LEXICON = {\"caring\",\"nurturing\",\"friendly\",\"warm\",\"kind\",\"gentle\",\"compassionate\",\"loving\",\"supportive\",\"affectionate\"}\n",
        "COMPETENCE_LEXICON = {\"skilled\",\"competent\",\"expert\",\"talented\",\"proficient\",\"capable\",\"strong\",\"efficient\",\"accomplished\"}\n",
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "def find_text_column(df):\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\",\"Description\"]\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "        lowcols = {col.lower(): col for col in df.columns}\n",
        "        if c.lower() in lowcols:\n",
        "            return lowcols[c.lower()]\n",
        "    string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "    if not string_cols:\n",
        "        raise KeyError(\"No string-like columns found\")\n",
        "    avg_len = {col: df[col].astype(str).map(len).mean() for col in string_cols}\n",
        "    return max(avg_len, key=avg_len.get)\n",
        "\n",
        "def extract_numbers(filename):\n",
        "    parts = str(filename).split('_')\n",
        "    if len(parts) >= 2:\n",
        "        try:\n",
        "            return int(parts[0]), int(parts[1])\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    return None, None\n",
        "\n",
        "def load_and_prepare(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    text_col = find_text_column(df)\n",
        "    df = df.copy()\n",
        "    df.rename(columns={text_col: \"caption\"}, inplace=True)\n",
        "    df[['first_num','second_num']] = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "    df['gender_label'] = df['first_num'].map(GENDER_MAP).fillna(\"unknown\")\n",
        "    df['race_label'] = df['second_num'].map(RACE_MAP).fillna(\"unknown\")\n",
        "    split_dataframes = {}\n",
        "    for first in [1,2]:\n",
        "        for second in range(1,7):\n",
        "            name = f\"df_{first}_{second}\"\n",
        "            sdf = df[(df['first_num']==first) & (df['second_num']==second)].copy().reset_index(drop=True)\n",
        "            if not sdf.empty:\n",
        "                sdf['group_label'] = f\"{sdf['gender_label'].iloc[0]}_{sdf['race_label'].iloc[0]}\"\n",
        "            else:\n",
        "                sdf['group_label'] = \"\"\n",
        "            split_dataframes[name] = sdf\n",
        "    return df, split_dataframes\n",
        "\n",
        "def compute_caption_embeddings(captions, batch_size=64):\n",
        "    embs = model.encode(list(captions), convert_to_numpy=True, show_progress_bar=False, batch_size=batch_size)\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    norms[norms==0] = 1.0\n",
        "    embs = embs / norms\n",
        "    return embs\n",
        "\n",
        "def compute_embedding_centroid(seeds):\n",
        "    if not seeds:\n",
        "        raise ValueError(\"Empty seeds\")\n",
        "    emb = model.encode(seeds, convert_to_numpy=True, show_progress_bar=False)\n",
        "    centroid = emb.mean(axis=0)\n",
        "    norm = np.linalg.norm(centroid)\n",
        "    if norm > 0:\n",
        "        centroid = centroid / norm\n",
        "    return centroid\n",
        "\n",
        "def cos_sim_to_centroid(caption_embs, centroid):\n",
        "    return np.dot(caption_embs, centroid)\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^\\w\\s']\", \" \", text)\n",
        "    tokens = [t for t in text.split() if t]\n",
        "    return tokens\n",
        "\n",
        "def lexicon_scores_for_caption(caption, warmth_lex=WARMTH_LEXICON, comp_lex=COMPETENCE_LEXICON):\n",
        "    tokens = simple_tokenize(caption)\n",
        "    token_count = len(tokens)\n",
        "    warmth_count = sum(1 for t in tokens if t in warmth_lex)\n",
        "    comp_count = sum(1 for t in tokens if t in comp_lex)\n",
        "    return {\"word_count\": token_count, \"warmth_count\": warmth_count, \"competence_count\": comp_count, \"warmth_rate\": warmth_count / token_count if token_count else 0.0, \"competence_rate\": comp_count / token_count if token_count else 0.0}\n",
        "\n",
        "def analyze_group_df(group_df, warmth_seeds=list(WARMTH_LEXICON), comp_seeds=list(COMPETENCE_LEXICON)):\n",
        "    if group_df is None or len(group_df)==0:\n",
        "        return pd.DataFrame(), None\n",
        "    lex_rows = [lexicon_scores_for_caption(c) for c in group_df['caption']]\n",
        "    lex_df = pd.DataFrame(lex_rows)\n",
        "    cap_embs = compute_caption_embeddings(group_df['caption'].tolist())\n",
        "    warmth_centroid = compute_embedding_centroid(warmth_seeds)\n",
        "    comp_centroid = compute_embedding_centroid(comp_seeds)\n",
        "    warmth_sim = cos_sim_to_centroid(cap_embs, warmth_centroid)\n",
        "    comp_sim = cos_sim_to_centroid(cap_embs, comp_centroid)\n",
        "    out = group_df.copy().reset_index(drop=True)\n",
        "    out = pd.concat([out, lex_df], axis=1)\n",
        "    out['warmth_emb_sim'] = warmth_sim\n",
        "    out['competence_emb_sim'] = comp_sim\n",
        "    return out, cap_embs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_df, split_dataframes = load_and_prepare(CSV_PATH)\n",
        "    analysis_results = {}\n",
        "    embeddings_store = {}\n",
        "    for name, sdf in split_dataframes.items():\n",
        "        if sdf is None or sdf.empty:\n",
        "            analysis_results[name] = pd.DataFrame()\n",
        "            embeddings_store[name] = None\n",
        "            continue\n",
        "        res_df, embs = analyze_group_df(sdf)\n",
        "        analysis_results[name] = res_df\n",
        "        embeddings_store[name] = embs\n",
        "\n",
        "    male_all = pd.concat([analysis_results.get(f\"df_1_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    female_all = pd.concat([analysis_results.get(f\"df_2_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    male_all = male_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "    female_all = female_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "    warmth_centroid = compute_embedding_centroid(list(WARMTH_LEXICON))\n",
        "    comp_centroid = compute_embedding_centroid(list(COMPETENCE_LEXICON))\n",
        "\n",
        "    outdir = \"/content/drive/MyDrive/image_captioning_bias/images/claude\"\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    jitter = 0.0\n",
        "\n",
        "    x_m = male_all['warmth_emb_sim'].values + np.random.normal(0, jitter, size=male_all.shape[0])\n",
        "    y_m = male_all['competence_emb_sim'].values + np.random.normal(0, jitter, size=male_all.shape[0])\n",
        "    x_f = female_all['warmth_emb_sim'].values + np.random.normal(0, jitter, size=female_all.shape[0])\n",
        "    y_f = female_all['competence_emb_sim'].values + np.random.normal(0, jitter, size=female_all.shape[0])\n",
        "\n",
        "    plt.scatter(x_m, y_m, alpha=0.6, label='male', s=sz1)\n",
        "    plt.scatter(x_f, y_f, alpha=0.6, label='female', s=sz1)\n",
        "\n",
        "    if len(x_m) >= 2 and np.nanstd(x_m) > 0:\n",
        "        coeffs_m = np.polyfit(x_m, y_m, 1)\n",
        "        xs = np.linspace(np.nanmin(np.concatenate([x_m,x_f])), np.nanmax(np.concatenate([x_m,x_f])), 200)\n",
        "        ys_m = coeffs_m[0]*xs + coeffs_m[1]\n",
        "        plt.plot(xs, ys_m, color='black', linestyle='-', linewidth=2, label=f'male fit (slope={coeffs_m[0]:.4f})')\n",
        "\n",
        "    if len(x_f) >= 2 and np.nanstd(x_f) > 0:\n",
        "        coeffs_f = np.polyfit(x_f, y_f, 1)\n",
        "        xs = np.linspace(np.nanmin(np.concatenate([x_m,x_f])), np.nanmax(np.concatenate([x_m,x_f])), 200)\n",
        "        ys_f = coeffs_f[0]*xs + coeffs_f[1]\n",
        "        plt.plot(xs, ys_f, color='gray', linestyle='--', linewidth=2, label=f'female fit (slope={coeffs_f[0]:.4f})')\n",
        "\n",
        "    cx_m, cy_m = np.mean(x_m), np.mean(y_m)\n",
        "    cx_f, cy_f = np.mean(x_f), np.mean(y_f)\n",
        "\n",
        "    plt.scatter([cx_m],[cy_m], marker='X', s=sz2, c='black')\n",
        "    plt.text(cx_m, cy_m, \"  male centroid\", fontsize=9)\n",
        "    plt.scatter([cx_f],[cy_f], marker='D', s=sz2, c='black')\n",
        "    plt.text(cx_f, cy_f, \"  female centroid\", fontsize=9)\n",
        "    plt.xlabel(\"Warmth similarity (embedding centroid)\")\n",
        "    plt.ylabel(\"Competence similarity (embedding centroid)\")\n",
        "    plt.title(\"Warmth vs Competence — Male vs Female (Claude)\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    outfig = os.path.join(outdir, \"claude_agg_male_vs_female_warmth_competence_with_slopes.png\")\n",
        "    plt.savefig(outfig, dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    SKIN_GROUPS = {\"light\":[1,2], \"mid\":[3,4], \"dark\":[5,6]}\n",
        "    skin_agg_results = {}\n",
        "    skin_embs = {}\n",
        "\n",
        "    for s in range(1,7):\n",
        "        name_m = f\"df_1_{s}\"\n",
        "        name_f = f\"df_2_{s}\"\n",
        "        combined_raw = pd.concat([analysis_results.get(name_m, pd.DataFrame()), analysis_results.get(name_f, pd.DataFrame())], ignore_index=True)\n",
        "        if combined_raw is None or combined_raw.empty:\n",
        "            continue\n",
        "        skin_res = combined_raw.copy()\n",
        "        skin_res['skin'] = f\"skin{s}\"\n",
        "        skin_agg_results[s] = skin_res\n",
        "    group_results = {}\n",
        "    group_embs = {}\n",
        "\n",
        "    for group_name, skin_list in SKIN_GROUPS.items():\n",
        "        dfs = []\n",
        "        for s in skin_list:\n",
        "            sdf = skin_agg_results.get(s)\n",
        "            if sdf is not None and not sdf.empty:\n",
        "                dfs.append(sdf)\n",
        "        if not dfs:\n",
        "            continue\n",
        "        combined = pd.concat(dfs, ignore_index=True)\n",
        "        combined = combined.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "        group_results[group_name] = combined\n",
        "    group_colors = {\"light\":\"#1f77b4\", \"mid\":\"#ff7f0e\", \"dark\":\"#2ca02c\"}\n",
        "    group_markers = {\"light\":\"o\", \"mid\":\"^\", \"dark\":\"s\"}\n",
        "    plt.figure(figsize=(9,7))\n",
        "\n",
        "   slopes = {}\n",
        "\n",
        "    for group_name in group_results.keys():\n",
        "        gdf = group_results[group_name]\n",
        "        x = gdf['warmth_emb_sim'].values\n",
        "        y = gdf['competence_emb_sim'].values\n",
        "        plt.scatter(x, y, label=group_name, alpha=0.6, s=sz1, color=group_colors.get(group_name,None))\n",
        "        if len(x) >= 2 and np.nanstd(x) > 0:\n",
        "            coeffs = np.polyfit(x, y, 1)\n",
        "            xs_line = np.linspace(float(np.nanmin(x)), float(np.nanmax(x)), 200)\n",
        "            ys_line = coeffs[0]*xs_line + coeffs[1]\n",
        "            plt.plot(xs_line, ys_line, color=group_colors.get(group_name,None), linewidth=2, label=f\"{group_name} fit (slope={coeffs[0]:.4f})\")\n",
        "            slopes[group_name] = float(coeffs[0])\n",
        "        cx = np.mean(x); cy = np.mean(y)\n",
        "        plt.scatter([cx],[cy], marker='X', s=sz2, color=group_colors.get(group_name,None))\n",
        "        plt.text(cx, cy, f\"  {group_name}\", fontsize=9)\n",
        "\n",
        "    plt.xlabel(\"Warmth similarity\")\n",
        "    plt.ylabel(\"Competence similarity\")\n",
        "    plt.title(\"Warmth vs Competence — Skin groups (light/mid/dark) (Claude)\")\n",
        "    plt.legend(title=\"skin group\", loc='best')\n",
        "    plt.tight_layout()\n",
        "    out_comb_warm = os.path.join(outdir, \"claude_combined_skingroups_warmth_competence_with_slopes.png\")\n",
        "    plt.savefig(out_comb_warm, dpi=300)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outfig)\n",
        "    print(\"Saved:\", out_comb_warm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "612wArGJqgqF"
      },
      "source": [
        "##Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY6nJ9Xmsyem"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/image_captioning_bias/gemini/captions.csv\"\n",
        "GENDER_MAP = {1: \"male\", 2: \"female\"}\n",
        "RACE_MAP = {1: \"skin1\", 2: \"skin2\", 3: \"skin3\", 4: \"skin4\", 5: \"skin5\", 6: \"skin6\"}\n",
        "WARMTH_LEXICON = {\"caring\",\"nurturing\",\"friendly\",\"warm\",\"kind\",\"gentle\",\"compassionate\",\"loving\",\"supportive\",\"affectionate\"}\n",
        "COMPETENCE_LEXICON = {\"skilled\",\"competent\",\"expert\",\"talented\",\"proficient\",\"capable\",\"strong\",\"efficient\",\"accomplished\"}\n",
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "def find_text_column(df):\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\",\"Description\"]\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "        lowcols = {col.lower(): col for col in df.columns}\n",
        "        if c.lower() in lowcols:\n",
        "            return lowcols[c.lower()]\n",
        "    string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "    if not string_cols:\n",
        "        raise KeyError(\"No string-like columns found\")\n",
        "    avg_len = {col: df[col].astype(str).map(len).mean() for col in string_cols}\n",
        "    return max(avg_len, key=avg_len.get)\n",
        "\n",
        "def extract_numbers(filename):\n",
        "    parts = str(filename).split('_')\n",
        "    if len(parts) >= 2:\n",
        "        try:\n",
        "            return int(parts[0]), int(parts[1])\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    return None, None\n",
        "\n",
        "def load_and_prepare(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    text_col = find_text_column(df)\n",
        "    df = df.copy()\n",
        "    df.rename(columns={text_col: \"caption\"}, inplace=True)\n",
        "    df[['first_num','second_num']] = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "    df['gender_label'] = df['first_num'].map(GENDER_MAP).fillna(\"unknown\")\n",
        "    df['race_label'] = df['second_num'].map(RACE_MAP).fillna(\"unknown\")\n",
        "    split_dataframes = {}\n",
        "    for first in [1,2]:\n",
        "        for second in range(1,7):\n",
        "            name = f\"df_{first}_{second}\"\n",
        "            sdf = df[(df['first_num']==first) & (df['second_num']==second)].copy().reset_index(drop=True)\n",
        "            if not sdf.empty:\n",
        "                sdf['group_label'] = f\"{sdf['gender_label'].iloc[0]}_{sdf['race_label'].iloc[0]}\"\n",
        "            else:\n",
        "                sdf['group_label'] = \"\"\n",
        "            split_dataframes[name] = sdf\n",
        "    return df, split_dataframes\n",
        "\n",
        "def compute_caption_embeddings(captions, batch_size=64):\n",
        "    embs = model.encode(list(captions), convert_to_numpy=True, show_progress_bar=False, batch_size=batch_size)\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    norms[norms==0] = 1.0\n",
        "    embs = embs / norms\n",
        "    return embs\n",
        "\n",
        "def compute_embedding_centroid(seeds):\n",
        "    if not seeds:\n",
        "        raise ValueError(\"Empty seeds\")\n",
        "    emb = model.encode(seeds, convert_to_numpy=True, show_progress_bar=False)\n",
        "    centroid = emb.mean(axis=0)\n",
        "    norm = np.linalg.norm(centroid)\n",
        "    if norm > 0:\n",
        "        centroid = centroid / norm\n",
        "    return centroid\n",
        "\n",
        "def cos_sim_to_centroid(caption_embs, centroid):\n",
        "    return np.dot(caption_embs, centroid)\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^\\w\\s']\", \" \", text)\n",
        "    tokens = [t for t in text.split() if t]\n",
        "    return tokens\n",
        "\n",
        "def lexicon_scores_for_caption(caption, warmth_lex=WARMTH_LEXICON, comp_lex=COMPETENCE_LEXICON):\n",
        "    tokens = simple_tokenize(caption)\n",
        "    token_count = len(tokens)\n",
        "    warmth_count = sum(1 for t in tokens if t in warmth_lex)\n",
        "    comp_count = sum(1 for t in tokens if t in comp_lex)\n",
        "    return {\"word_count\": token_count, \"warmth_count\": warmth_count, \"competence_count\": comp_count, \"warmth_rate\": warmth_count / token_count if token_count else 0.0, \"competence_rate\": comp_count / token_count if token_count else 0.0}\n",
        "\n",
        "def analyze_group_df(group_df, warmth_seeds=list(WARMTH_LEXICON), comp_seeds=list(COMPETENCE_LEXICON)):\n",
        "    if group_df is None or len(group_df)==0:\n",
        "        return pd.DataFrame(), None\n",
        "    lex_rows = [lexicon_scores_for_caption(c) for c in group_df['caption']]\n",
        "    lex_df = pd.DataFrame(lex_rows)\n",
        "    cap_embs = compute_caption_embeddings(group_df['caption'].tolist())\n",
        "    warmth_centroid = compute_embedding_centroid(warmth_seeds)\n",
        "    comp_centroid = compute_embedding_centroid(comp_seeds)\n",
        "    warmth_sim = cos_sim_to_centroid(cap_embs, warmth_centroid)\n",
        "    comp_sim = cos_sim_to_centroid(cap_embs, comp_centroid)\n",
        "    out = group_df.copy().reset_index(drop=True)\n",
        "    out = pd.concat([out, lex_df], axis=1)\n",
        "    out['warmth_emb_sim'] = warmth_sim\n",
        "    out['competence_emb_sim'] = comp_sim\n",
        "    return out, cap_embs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_df, split_dataframes = load_and_prepare(CSV_PATH)\n",
        "    analysis_results = {}\n",
        "    embeddings_store = {}\n",
        "\n",
        "    for name, sdf in split_dataframes.items():\n",
        "        if sdf is None or sdf.empty:\n",
        "            analysis_results[name] = pd.DataFrame()\n",
        "            embeddings_store[name] = None\n",
        "            continue\n",
        "        res_df, embs = analyze_group_df(sdf)\n",
        "        analysis_results[name] = res_df\n",
        "        embeddings_store[name] = embs\n",
        "\n",
        "    male_all = pd.concat([analysis_results.get(f\"df_1_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    female_all = pd.concat([analysis_results.get(f\"df_2_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    male_all = male_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "    female_all = female_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "    warmth_centroid = compute_embedding_centroid(list(WARMTH_LEXICON))\n",
        "    comp_centroid = compute_embedding_centroid(list(COMPETENCE_LEXICON))\n",
        "    outdir = \"/content/drive/MyDrive/image_captioning_bias/images/gemini\"\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    jitter = 0.0\n",
        "\n",
        "    x_m = male_all['warmth_emb_sim'].values + np.random.normal(0, jitter, size=male_all.shape[0])\n",
        "    y_m = male_all['competence_emb_sim'].values + np.random.normal(0, jitter, size=male_all.shape[0])\n",
        "    x_f = female_all['warmth_emb_sim'].values + np.random.normal(0, jitter, size=female_all.shape[0])\n",
        "    y_f = female_all['competence_emb_sim'].values + np.random.normal(0, jitter, size=female_all.shape[0])\n",
        "\n",
        "    plt.scatter(x_m, y_m, alpha=0.6, label='male', s=sz1)\n",
        "    plt.scatter(x_f, y_f, alpha=0.6, label='female', s=sz1)\n",
        "\n",
        "    if len(x_m) >= 2 and np.nanstd(x_m) > 0:\n",
        "        coeffs_m = np.polyfit(x_m, y_m, 1)\n",
        "        xs = np.linspace(np.nanmin(np.concatenate([x_m,x_f])), np.nanmax(np.concatenate([x_m,x_f])), 200)\n",
        "        ys_m = coeffs_m[0]*xs + coeffs_m[1]\n",
        "        plt.plot(xs, ys_m, color='black', linestyle='-', linewidth=2, label=f'male fit (slope={coeffs_m[0]:.4f})')\n",
        "    if len(x_f) >= 2 and np.nanstd(x_f) > 0:\n",
        "        coeffs_f = np.polyfit(x_f, y_f, 1)\n",
        "        xs = np.linspace(np.nanmin(np.concatenate([x_m,x_f])), np.nanmax(np.concatenate([x_m,x_f])), 200)\n",
        "        ys_f = coeffs_f[0]*xs + coeffs_f[1]\n",
        "        plt.plot(xs, ys_f, color='gray', linestyle='--', linewidth=2, label=f'female fit (slope={coeffs_f[0]:.4f})')\n",
        "\n",
        "    cx_m, cy_m = np.mean(x_m), np.mean(y_m)\n",
        "    cx_f, cy_f = np.mean(x_f), np.mean(y_f)\n",
        "\n",
        "    plt.scatter([cx_m],[cy_m], marker='X', s=sz2, c='black')\n",
        "    plt.text(cx_m, cy_m, \"  male centroid\", fontsize=9)\n",
        "    plt.scatter([cx_f],[cy_f], marker='D', s=sz2, c='black')\n",
        "    plt.text(cx_f, cy_f, \"  female centroid\", fontsize=9)\n",
        "    plt.xlabel(\"Warmth similarity (embedding centroid)\")\n",
        "    plt.ylabel(\"Competence similarity (embedding centroid)\")\n",
        "    plt.title(\"Warmth vs Competence — Male vs Female (Gemini)\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    outfig = os.path.join(outdir, \"gemini_agg_male_vs_female_warmth_competence_with_slopes.png\")\n",
        "    plt.savefig(outfig, dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    SKIN_GROUPS = {\"light\":[1,2], \"mid\":[3,4], \"dark\":[5,6]}\n",
        "    skin_agg_results = {}\n",
        "    skin_embs = {}\n",
        "\n",
        "    for s in range(1,7):\n",
        "        name_m = f\"df_1_{s}\"\n",
        "        name_f = f\"df_2_{s}\"\n",
        "        combined_raw = pd.concat([analysis_results.get(name_m, pd.DataFrame()), analysis_results.get(name_f, pd.DataFrame())], ignore_index=True)\n",
        "        if combined_raw is None or combined_raw.empty:\n",
        "            continue\n",
        "        skin_res = combined_raw.copy()\n",
        "        skin_res['skin'] = f\"skin{s}\"\n",
        "        skin_agg_results[s] = skin_res\n",
        "\n",
        "    group_results = {}\n",
        "    group_embs = {}\n",
        "\n",
        "    for group_name, skin_list in SKIN_GROUPS.items():\n",
        "        dfs = []\n",
        "        for s in skin_list:\n",
        "            sdf = skin_agg_results.get(s)\n",
        "            if sdf is not None and not sdf.empty:\n",
        "                dfs.append(sdf)\n",
        "        if not dfs:\n",
        "            continue\n",
        "        combined = pd.concat(dfs, ignore_index=True)\n",
        "        combined = combined.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "        group_results[group_name] = combined\n",
        "\n",
        "    group_colors = {\"light\":\"#1f77b4\", \"mid\":\"#ff7f0e\", \"dark\":\"#2ca02c\"}\n",
        "    group_markers = {\"light\":\"o\", \"mid\":\"^\", \"dark\":\"s\"}\n",
        "    plt.figure(figsize=(9,7))\n",
        "    slopes = {}\n",
        "\n",
        "    for group_name in group_results.keys():\n",
        "        gdf = group_results[group_name]\n",
        "        x = gdf['warmth_emb_sim'].values\n",
        "        y = gdf['competence_emb_sim'].values\n",
        "        plt.scatter(x, y, label=group_name, alpha=0.6, s=sz1, color=group_colors.get(group_name,None))\n",
        "        if len(x) >= 2 and np.nanstd(x) > 0:\n",
        "            coeffs = np.polyfit(x, y, 1)\n",
        "            xs_line = np.linspace(float(np.nanmin(x)), float(np.nanmax(x)), 200)\n",
        "            ys_line = coeffs[0]*xs_line + coeffs[1]\n",
        "            plt.plot(xs_line, ys_line, color=group_colors.get(group_name,None), linewidth=2, label=f\"{group_name} fit (slope={coeffs[0]:.4f})\")\n",
        "            slopes[group_name] = float(coeffs[0])\n",
        "        cx = np.mean(x); cy = np.mean(y)\n",
        "        plt.scatter([cx],[cy], marker='X', s=sz2, color=group_colors.get(group_name,None))\n",
        "        plt.text(cx, cy, f\"  {group_name}\", fontsize=9)\n",
        "\n",
        "    plt.xlabel(\"Warmth similarity\")\n",
        "    plt.ylabel(\"Competence similarity\")\n",
        "    plt.title(\"Warmth vs Competence — Skin groups (light/mid/dark) (Gemini)\")\n",
        "    plt.legend(title=\"skin group\", loc='best')\n",
        "    plt.tight_layout()\n",
        "    out_comb_warm = os.path.join(outdir, \"gemini_combined_skingroups_warmth_competence_with_slopes.png\")\n",
        "    plt.savefig(out_comb_warm, dpi=300)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outfig)\n",
        "    print(\"Saved:\", out_comb_warm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "CSV_PATHS = {\n",
        "    \"chatgpt\": \"/content/drive/MyDrive/image_captioning_bias/chatgpt/captions.csv\",\n",
        "    \"claude\":  \"/content/drive/MyDrive/image_captioning_bias/claude/captions.csv\",\n",
        "    \"gemini\":  \"/content/drive/MyDrive/image_captioning_bias/gemini/captions.csv\",\n",
        "}\n",
        "OUTDIR = \"/content/drive/MyDrive/image_captioning_bias/processed\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "GENDER_MAP = {1: \"male\", 2: \"female\"}\n",
        "RACE_MAP = {1: \"skin1\", 2: \"skin2\", 3: \"skin3\", 4: \"skin4\", 5: \"skin5\", 6: \"skin6\"}\n",
        "WARMTH_LEXICON = {\"caring\",\"nurturing\",\"friendly\",\"warm\",\"kind\",\"gentle\",\"compassionate\",\"loving\",\"supportive\",\"affectionate\"}\n",
        "COMPETENCE_LEXICON = {\"skilled\",\"competent\",\"expert\",\"talented\",\"proficient\",\"capable\",\"strong\",\"efficient\",\"accomplished\"}\n",
        "\n",
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "print(\"Loading embedding model:\", EMBED_MODEL_NAME)\n",
        "model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "def find_text_column(df):\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\",\"Description\"]\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "        lowcols = {col.lower(): col for col in df.columns}\n",
        "        if c.lower() in lowcols:\n",
        "            return lowcols[c.lower()]\n",
        "    string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "    if not string_cols:\n",
        "        raise KeyError(\"No string-like columns found\")\n",
        "    avg_len = {col: df[col].astype(str).map(len).mean() for col in string_cols}\n",
        "    return max(avg_len, key=avg_len.get)\n",
        "\n",
        "def extract_numbers(filename):\n",
        "    parts = str(filename).split('_')\n",
        "    if len(parts) >= 2:\n",
        "        try:\n",
        "            return int(parts[0]), int(parts[1])\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    return None, None\n",
        "\n",
        "def load_and_prepare(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    text_col = find_text_column(df)\n",
        "    df = df.copy()\n",
        "    df.rename(columns={text_col: \"caption\"}, inplace=True)\n",
        "    df[['first_num','second_num']] = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "    df['gender_label'] = df['first_num'].map(GENDER_MAP).fillna(\"unknown\")\n",
        "    df['race_label'] = df['second_num'].map(RACE_MAP).fillna(\"unknown\")\n",
        "    split_dataframes = {}\n",
        "    for first in [1,2]:\n",
        "        for second in range(1,7):\n",
        "            name = f\"df_{first}_{second}\"\n",
        "            sdf = df[(df['first_num']==first) & (df['second_num']==second)].copy().reset_index(drop=True)\n",
        "            if not sdf.empty:\n",
        "                sdf['group_label'] = f\"{sdf['gender_label'].iloc[0]}_{sdf['race_label'].iloc[0]}\"\n",
        "            else:\n",
        "                sdf['group_label'] = \"\"\n",
        "            split_dataframes[name] = sdf\n",
        "    return df, split_dataframes\n",
        "\n",
        "def compute_caption_embeddings(captions, batch_size=64):\n",
        "    embs = model.encode(list(captions), convert_to_numpy=True, show_progress_bar=False, batch_size=batch_size)\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    norms[norms==0] = 1.0\n",
        "    embs = embs / norms\n",
        "    return embs\n",
        "\n",
        "def compute_embedding_centroid(seeds):\n",
        "    if not seeds:\n",
        "        raise ValueError(\"Empty seeds\")\n",
        "    emb = model.encode(seeds, convert_to_numpy=True, show_progress_bar=False)\n",
        "    centroid = emb.mean(axis=0)\n",
        "    norm = np.linalg.norm(centroid)\n",
        "    if norm > 0:\n",
        "        centroid = centroid / norm\n",
        "    return centroid\n",
        "\n",
        "def cos_sim_to_centroid(caption_embs, centroid):\n",
        "    return np.dot(caption_embs, centroid)\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^\\w\\s']\", \" \", text)\n",
        "    tokens = [t for t in text.split() if t]\n",
        "    return tokens\n",
        "\n",
        "def lexicon_scores_for_caption(caption, warmth_lex=WARMTH_LEXICON, comp_lex=COMPETENCE_LEXICON):\n",
        "    tokens = simple_tokenize(caption)\n",
        "    token_count = len(tokens)\n",
        "    warmth_count = sum(1 for t in tokens if t in warmth_lex)\n",
        "    comp_count = sum(1 for t in tokens if t in comp_lex)\n",
        "    return {\"word_count\": token_count,\n",
        "            \"warmth_count\": warmth_count,\n",
        "            \"competence_count\": comp_count,\n",
        "            \"warmth_rate\": warmth_count / token_count if token_count else 0.0,\n",
        "            \"competence_rate\": comp_count / token_count if token_count else 0.0}\n",
        "\n",
        "def analyze_group_df(group_df, warmth_seeds=list(WARMTH_LEXICON), comp_seeds=list(COMPETENCE_LEXICON)):\n",
        "    if group_df is None or len(group_df)==0:\n",
        "        return pd.DataFrame(), None\n",
        "    lex_rows = [lexicon_scores_for_caption(c) for c in group_df['caption']]\n",
        "    lex_df = pd.DataFrame(lex_rows)\n",
        "    cap_embs = compute_caption_embeddings(group_df['caption'].tolist())\n",
        "    warmth_centroid = compute_embedding_centroid(warmth_seeds)\n",
        "    comp_centroid = compute_embedding_centroid(comp_seeds)\n",
        "    warmth_sim = cos_sim_to_centroid(cap_embs, warmth_centroid)\n",
        "    comp_sim = cos_sim_to_centroid(cap_embs, comp_centroid)\n",
        "    out = group_df.copy().reset_index(drop=True)\n",
        "    out = pd.concat([out, lex_df], axis=1)\n",
        "    out['warmth_emb_sim'] = warmth_sim\n",
        "    out['competence_emb_sim'] = comp_sim\n",
        "    return out, cap_embs\n",
        "\n",
        "def process_csv_and_extract(csv_path):\n",
        "    base_df, split_dataframes = load_and_prepare(csv_path)\n",
        "    analysis_results = {}\n",
        "    embeddings_store = {}\n",
        "    for name, sdf in split_dataframes.items():\n",
        "        if sdf is None or sdf.empty:\n",
        "            analysis_results[name] = pd.DataFrame()\n",
        "            embeddings_store[name] = None\n",
        "            continue\n",
        "        res_df, embs = analyze_group_df(sdf)\n",
        "        analysis_results[name] = res_df\n",
        "        embeddings_store[name] = embs\n",
        "\n",
        "    male_all = pd.concat([analysis_results.get(f\"df_1_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    female_all = pd.concat([analysis_results.get(f\"df_2_{i}\", pd.DataFrame()) for i in range(1,7)], ignore_index=True)\n",
        "    male_all = male_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "    female_all = female_all.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "\n",
        "    skin_agg_results = {}\n",
        "    for s in range(1,7):\n",
        "        name_m = f\"df_1_{s}\"\n",
        "        name_f = f\"df_2_{s}\"\n",
        "        combined_raw = pd.concat([analysis_results.get(name_m, pd.DataFrame()), analysis_results.get(name_f, pd.DataFrame())], ignore_index=True)\n",
        "        if combined_raw is None or combined_raw.empty:\n",
        "            continue\n",
        "        skin_res = combined_raw.copy()\n",
        "        skin_res['skin'] = f\"skin{s}\"\n",
        "        skin_agg_results[s] = skin_res\n",
        "\n",
        "    group_results = {}\n",
        "    SKIN_GROUPS = {\"light\":[1,2], \"mid\":[3,4], \"dark\":[5,6]}\n",
        "    for group_name, skin_list in SKIN_GROUPS.items():\n",
        "        dfs = []\n",
        "        for s in skin_list:\n",
        "            sdf = skin_agg_results.get(s)\n",
        "            if sdf is not None and not sdf.empty:\n",
        "                dfs.append(sdf)\n",
        "        if not dfs:\n",
        "            continue\n",
        "        combined = pd.concat(dfs, ignore_index=True)\n",
        "        combined = combined.dropna(subset=['warmth_emb_sim','competence_emb_sim'])\n",
        "        group_results[group_name] = combined\n",
        "\n",
        "    return {\n",
        "        \"male_all\": male_all,\n",
        "        \"female_all\": female_all,\n",
        "        \"group_results\": group_results\n",
        "    }\n",
        "\n",
        "processed = {}\n",
        "for model_name, path in CSV_PATHS.items():\n",
        "    print(\"Processing:\", model_name, \"from\", path)\n",
        "    processed[model_name] = process_csv_and_extract(path)\n",
        "\n",
        "save_path = os.path.join(OUTDIR, \"processed_data.pkl\")\n",
        "with open(save_path, \"wb\") as f:\n",
        "    pickle.dump(processed, f, protocol=4)\n",
        "\n",
        "print(\"Saved processed data to:\", save_path)\n"
      ],
      "metadata": {
        "id": "z2JOTLWZZ1z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fS7RFqTniQ59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "PICKLE_PATH = \"/content/drive/MyDrive/image_captioning_bias/processed/processed_data.pkl\"\n",
        "OUTDIR = \"/content/drive/MyDrive/image_captioning_bias/images/combined\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "TITLE_FS = 15\n",
        "AXIS_LABEL_FS = 15\n",
        "AXIS_TICK_FS = 15\n",
        "LEGEND_FS = 15\n",
        "CENTROID_TEXT_FS = 13\n",
        "\n",
        "sz1 = 10\n",
        "sz2 = 80\n",
        "\n",
        "SKIN_GROUPS = {\"light\":[1,2], \"mid\":[3,4], \"dark\":[5,6]}\n",
        "group_colors = {\"light\":\"#1f77b4\", \"mid\":\"#ff7f0e\", \"dark\":\"#2ca02c\"}\n",
        "group_markers = {\"light\":\"o\", \"mid\":\"^\", \"dark\":\"s\"}\n",
        "\n",
        "male_color = \"tab:blue\"\n",
        "female_color = \"tab:orange\"\n",
        "male_centroid_marker = \"D\"\n",
        "female_centroid_marker = \"D\"\n",
        "\n",
        "with open(PICKLE_PATH, \"rb\") as f:\n",
        "    processed = pickle.load(f)\n",
        "\n",
        "model_order = [\"chatgpt\", \"claude\", \"gemini\"]\n",
        "\n",
        "def compute_global_limits_for_gender(processed, model_order):\n",
        "    all_x, all_y = [], []\n",
        "    for m in model_order:\n",
        "        p = processed.get(m)\n",
        "        if p is None:\n",
        "            continue\n",
        "        for k in [\"male_all\", \"female_all\"]:\n",
        "            df = p.get(k)\n",
        "            if df is not None and not getattr(df, \"empty\", True):\n",
        "                all_x.append(df[\"warmth_emb_sim\"].values)\n",
        "                all_y.append(df[\"competence_emb_sim\"].values)\n",
        "\n",
        "    x = np.concatenate(all_x)\n",
        "    y = np.concatenate(all_y)\n",
        "    pad_x = (x.max() - x.min()) * 0.08\n",
        "    pad_y = (y.max() - y.min()) * 0.08\n",
        "    return (x.min()-pad_x, x.max()+pad_x), (y.min()-pad_y, y.max()+pad_y)\n",
        "\n",
        "\n",
        "def compute_global_limits_for_groups(processed, model_order):\n",
        "    all_x, all_y = [], []\n",
        "    for m in model_order:\n",
        "        grp = processed[m].get(\"group_results\", {})\n",
        "        for v in grp.values():\n",
        "            if v is not None and not getattr(v, \"empty\", True):\n",
        "                all_x.append(v[\"warmth_emb_sim\"].values)\n",
        "                all_y.append(v[\"competence_emb_sim\"].values)\n",
        "\n",
        "    x = np.concatenate(all_x)\n",
        "    y = np.concatenate(all_y)\n",
        "    pad_x = (x.max() - x.min()) * 0.08\n",
        "    pad_y = (y.max() - y.min()) * 0.08\n",
        "    return (x.min()-pad_x, x.max()+pad_x), (y.min()-pad_y, y.max()+pad_y)\n",
        "\n",
        "from matplotlib.transforms import Bbox\n",
        "def remove_nearest_ticks(ax, x_centers=None, y_centers=None):\n",
        "    if x_centers is not None:\n",
        "        xticks = ax.get_xticks()\n",
        "        xlabels = [lab.get_text() for lab in ax.get_xticklabels()]\n",
        "        for xc in x_centers:\n",
        "            idx = np.argmin(np.abs(xticks - xc))\n",
        "            if idx < len(xlabels):\n",
        "                xlabels[idx] = \"\"\n",
        "        ax.set_xticklabels(xlabels)\n",
        "\n",
        "    if y_centers is not None:\n",
        "        yticks = ax.get_yticks()\n",
        "        ylabels = [str(tick) if tick in yticks else \"\" for tick in yticks]\n",
        "        for yc in y_centers:\n",
        "            idx = np.argmin(np.abs(yticks - yc))\n",
        "            if idx < len(ylabels):\n",
        "                ylabels[idx] = \"\"\n",
        "        ax.set_yticklabels(ylabels)\n",
        "\n",
        "def remove_specific_ticks(ax, x_remove=None, y_remove=None):\n",
        "\n",
        "    ax.figure.canvas.draw()\n",
        "\n",
        "    if x_remove is not None:\n",
        "        xticks = ax.get_xticks()\n",
        "        xlabels = [lab.get_text() for lab in ax.get_xticklabels()]\n",
        "        if not any(xlabels):\n",
        "            xlabels = [f\"{tick:.3f}\" for tick in xticks]\n",
        "\n",
        "        for x_val in x_remove:\n",
        "            idx = np.argmin(np.abs(xticks - x_val))\n",
        "            if idx < len(xlabels):\n",
        "                xlabels[idx] = \"\"\n",
        "        ax.set_xticklabels(xlabels)\n",
        "\n",
        "    if y_remove is not None:\n",
        "        yticks = ax.get_yticks()\n",
        "        ylabels = [lab.get_text() for lab in ax.get_yticklabels()]\n",
        "        if not any(ylabels):\n",
        "            ylabels = [f\"{tick:.3f}\" for tick in yticks]\n",
        "\n",
        "        for y_val in y_remove:\n",
        "            idx = np.argmin(np.abs(yticks - y_val))\n",
        "            if idx < len(ylabels):\n",
        "                ylabels[idx] = \"\"\n",
        "        ax.set_yticklabels(ylabels)\n",
        "\n",
        "def add_staggered_x_labels(ax, x_values, ylim, fontsize=13):\n",
        "\n",
        "    sorted_x = sorted(x_values)\n",
        "\n",
        "    y_range = ylim[1] - ylim[0]\n",
        "    base_offset = 0.02 * y_range\n",
        "    increment = 0.030 * y_range\n",
        "\n",
        "    for i, x_val in enumerate(sorted_x):\n",
        "        y_pos = ylim[0] - base_offset - (i * increment)\n",
        "\n",
        "        ax.text(x_val, y_pos, f\"{x_val:.3f}\",\n",
        "                fontsize=fontsize,\n",
        "                ha=\"center\",\n",
        "                va=\"top\")\n",
        "\n",
        "xlim, ylim = compute_global_limits_for_gender(processed, model_order)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n",
        "\n",
        "for idx, (ax, mname) in enumerate(zip(axes, model_order)):\n",
        "    p = processed[mname]\n",
        "    male, female = p[\"male_all\"], p[\"female_all\"]\n",
        "\n",
        "    x_m, y_m = male[\"warmth_emb_sim\"], male[\"competence_emb_sim\"]\n",
        "    x_f, y_f = female[\"warmth_emb_sim\"], female[\"competence_emb_sim\"]\n",
        "\n",
        "    ax.scatter(x_m, y_m, s=sz1, alpha=0.6, color=male_color)\n",
        "    ax.scatter(x_f, y_f, s=sz1, alpha=0.6, color=female_color)\n",
        "\n",
        "    slopes = {}\n",
        "    for label, x, y, c, ls in [\n",
        "        (\"Male\", x_m, y_m, \"black\", \"-\"),\n",
        "        (\"Female\", x_f, y_f, \"gray\", \"--\")\n",
        "    ]:\n",
        "        if len(x) > 1 and np.std(x) > 0:\n",
        "            b = np.polyfit(x, y, 1)\n",
        "            slopes[label] = b[0]\n",
        "            xs = np.linspace(*xlim, 200)\n",
        "            ax.plot(xs, b[0]*xs + b[1], color=c, lw=2, ls=ls)\n",
        "\n",
        "    mx, my = np.mean(x_m), np.mean(y_m)\n",
        "    fx, fy = np.mean(x_f), np.mean(y_f)\n",
        "\n",
        "    ax.scatter(mx, my, marker=male_centroid_marker, s=sz2,\n",
        "               color=male_color, edgecolors='black', linewidths=1.5, zorder=2)\n",
        "    ax.scatter(fx, fy, marker=female_centroid_marker, s=sz2,\n",
        "               color=female_color, edgecolors='black', linewidths=1.5, zorder=2)\n",
        "\n",
        "    add_staggered_x_labels(ax, [mx, fx], ylim, fontsize=CENTROID_TEXT_FS)\n",
        "\n",
        "    ax.vlines(mx, ylim[0], my, colors=male_color, linestyles=\":\", lw=1)\n",
        "    ax.hlines(my, xlim[0], mx, colors=male_color, linestyles=\":\", lw=1)\n",
        "\n",
        "    ax.vlines(fx, ylim[0], fy, colors=female_color, linestyles=\":\", lw=1)\n",
        "    ax.hlines(fy, xlim[0], fx, colors=female_color, linestyles=\":\", lw=1)\n",
        "\n",
        "\n",
        "    ax.text(xlim[0], my, f\"{my:.3f}\", fontsize=CENTROID_TEXT_FS,\n",
        "            ha=\"right\", va=\"center\")\n",
        "\n",
        "\n",
        "    ax.text(xlim[0], fy, f\"{fy:.3f}\", fontsize=CENTROID_TEXT_FS,\n",
        "            ha=\"right\", va=\"center\")\n",
        "\n",
        "\n",
        "    ax.legend(\n",
        "        handles=[\n",
        "            Line2D(\n",
        "                [0],[0], color=\"black\", lw=2,\n",
        "                label=f\"Male fit: {slopes.get('Male', np.nan):.3f}\"\n",
        "            ),\n",
        "            Line2D(\n",
        "                [0],[0], color=\"gray\", lw=2, ls=\"--\",\n",
        "                label=f\"Female fit: {slopes.get('Female', np.nan):.3f}\"\n",
        "            )\n",
        "        ],\n",
        "        loc=\"upper left\",\n",
        "        fontsize=LEGEND_FS,\n",
        "        frameon=True\n",
        "    )\n",
        "\n",
        "    ax.set_title(mname.capitalize(), fontsize=TITLE_FS)\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)\n",
        "    ax.tick_params(labelsize=AXIS_TICK_FS)\n",
        "\n",
        "    if idx == 0:  # ChatGPT\n",
        "        remove_specific_ticks(ax, x_remove=[0.1, 0.2], y_remove=[0.1])\n",
        "    elif idx == 1:  # Claude\n",
        "        remove_specific_ticks(ax, x_remove=[], y_remove=[0.1])\n",
        "    elif idx == 2:  # Gemini\n",
        "        remove_specific_ticks(ax, x_remove=[], y_remove=[0.1])\n",
        "\n",
        "\n",
        "axes[0].set_ylabel(\"Competence similarity\", fontsize=AXIS_LABEL_FS)\n",
        "for ax in axes:\n",
        "    ax.set_xlabel(\"Warmth similarity\", fontsize=AXIS_LABEL_FS)\n",
        "\n",
        "legend_handles_gender = [\n",
        "    Line2D([0],[0], marker='o', color='w', markerfacecolor=male_color, label=\"Male (points)\"),\n",
        "    Line2D([0],[0], marker='o', color='w', markerfacecolor=female_color, label=\"Female (points)\"),\n",
        "    Line2D([0],[0], color=\"black\", lw=2, label=\"Male fit\"),\n",
        "    Line2D([0],[0], color=\"gray\", lw=2, ls=\"--\", label=\"Female fit\"),\n",
        "    Line2D([0],[0], marker=male_centroid_marker, color='w', markerfacecolor=male_color, label=\"Male centroid\"),\n",
        "    Line2D([0],[0], marker=female_centroid_marker, color='w', markerfacecolor=female_color, label=\"Female centroid\"),\n",
        "]\n",
        "\n",
        "fig.legend(\n",
        "    handles=legend_handles_gender,\n",
        "    loc=\"upper center\",\n",
        "    ncol=6,\n",
        "    fontsize=LEGEND_FS,\n",
        "    frameon=True,\n",
        "    bbox_to_anchor=(0.5, 1.01)\n",
        ")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.savefig(os.path.join(OUTDIR, \"combined_male_vs_female_three_models.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "xlim2, ylim2 = compute_global_limits_for_groups(processed, model_order)\n",
        "\n",
        "fig2, axes2 = plt.subplots(1, 3, figsize=(18, 7), sharex=True, sharey=True)\n",
        "\n",
        "for ax, mname in zip(axes2, model_order):\n",
        "    grp = processed[mname][\"group_results\"]\n",
        "    slopes = {}\n",
        "\n",
        "    for g in [\"light\", \"mid\", \"dark\"]:\n",
        "        df = grp[g]\n",
        "        x, y = df[\"warmth_emb_sim\"], df[\"competence_emb_sim\"]\n",
        "        ax.scatter(x, y, s=sz1, alpha=0.6, color=group_colors[g])\n",
        "\n",
        "        if len(x) > 1 and np.std(x) > 0:\n",
        "            b = np.polyfit(x, y, 1)\n",
        "            slopes[g] = b[0]\n",
        "            xs = np.linspace(*xlim2, 200)\n",
        "            ax.plot(xs, b[0]*xs + b[1], lw=2, color=group_colors[g])\n",
        "\n",
        "        cx, cy = np.mean(x), np.mean(y)\n",
        "        ax.scatter(cx, cy, s=sz2, marker=\"D\",\n",
        "                   color=group_colors[g], edgecolors='black',\n",
        "                   linewidths=1.5, zorder=2)\n",
        "\n",
        "        print(cx, cy, mname, g)\n",
        "\n",
        "    ax.legend(\n",
        "        handles=[\n",
        "            Line2D([0],[0], color=group_colors[\"light\"], lw=2, label=f\"Light fit: {slopes.get('light', np.nan):.3f}\"),\n",
        "            Line2D([0],[0], color=group_colors[\"mid\"], lw=2, label=f\"Mid fit: {slopes.get('mid', np.nan):.3f}\"),\n",
        "            Line2D([0],[0], color=group_colors[\"dark\"], lw=2, label=f\"Dark fit: {slopes.get('dark', np.nan):.3f}\")\n",
        "        ],\n",
        "        loc=\"upper left\",\n",
        "        fontsize=LEGEND_FS,\n",
        "        frameon=True\n",
        "    )\n",
        "\n",
        "    ax.set_title(mname.capitalize(), fontsize=TITLE_FS)\n",
        "    ax.set_xlim(xlim2)\n",
        "    ax.set_ylim(ylim2)\n",
        "    ax.tick_params(labelsize=AXIS_TICK_FS)\n",
        "\n",
        "axes2[0].set_ylabel(\"Competence similarity\", fontsize=AXIS_LABEL_FS)\n",
        "for ax in axes2:\n",
        "    ax.set_xlabel(\"Warmth similarity\", fontsize=AXIS_LABEL_FS)\n",
        "\n",
        "legend_handles_groups = []\n",
        "for g in [\"light\", \"mid\", \"dark\"]:\n",
        "    legend_handles_groups.extend([\n",
        "        Line2D([0],[0], marker='o', color='w', markerfacecolor=group_colors[g], label=f\"{g} points\"),\n",
        "        Line2D([0],[0], color=group_colors[g], lw=2, label=f\"{g} fit\"),\n",
        "        Line2D([0],[0], marker='X', color='w', markerfacecolor=group_colors[g], label=f\"{g} centroid\"),\n",
        "    ])\n",
        "\n",
        "fig2.legend(\n",
        "    handles=legend_handles_groups,\n",
        "    loc=\"upper center\",\n",
        "    ncol=9,\n",
        "    fontsize=LEGEND_FS-1,\n",
        "    frameon=True,\n",
        "    bbox_to_anchor=(0.5, 0.90)\n",
        ")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.85])\n",
        "plt.savefig(os.path.join(OUTDIR, \"combined_skingroups_three_models.png\"), dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3ehPtsFl3eah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAHc8bxY7rz2"
      },
      "source": [
        "# Other metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob4uQqvTmHlM"
      },
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0aGEwhEnz_g"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a6tgj3T37E0"
      },
      "outputs": [],
      "source": [
        "import os, glob, re, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cdist\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/image_captioning_bias\"\n",
        "OUT_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "DEFAULT_MODEL_FILES = [\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/chatgpt/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/claude/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/gemini/captions.csv\"\n",
        "]\n",
        "MODEL_NAMES = [\"chatgpt\", \"claude\", \"gemini\"]\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "def secondnum_to_skingroup(second):\n",
        "    try:\n",
        "        s = int(second)\n",
        "    except Exception:\n",
        "        return \"unknown\"\n",
        "    if s in (1,2):\n",
        "        return \"light\"\n",
        "    if s in (3,4):\n",
        "        return \"mid\"\n",
        "    if s in (5,6):\n",
        "        return \"dark\"\n",
        "    return \"unknown\"\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\"]\n",
        "    text_col = None\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            text_col = c; break\n",
        "    if text_col is None:\n",
        "        string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "        if not string_cols:\n",
        "            raise KeyError(\"no text-like column\")\n",
        "        avglen = {col: df[col].astype(str).map(len).mean() for col in string_cols}\n",
        "        text_col = max(avglen, key=avglen.get)\n",
        "    if 'Filename' not in df.columns and 'filename' in df.columns:\n",
        "        df.rename(columns={'filename':'Filename'}, inplace=True)\n",
        "    df = df.copy()\n",
        "    df.rename(columns={text_col: \"Caption\"}, inplace=True)\n",
        "    df['Caption_norm'] = df['Caption'].fillna(\"\").astype(str).str.strip()\n",
        "    def extract_numbers(fname):\n",
        "        try:\n",
        "            parts = str(fname).split('_')\n",
        "            return int(parts[0]), int(parts[1])\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    if 'Filename' in df.columns:\n",
        "        nums = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "        df[['first_num','second_num']] = nums\n",
        "    return df[['Filename','Caption','Caption_norm','first_num','second_num']]\n",
        "\n",
        "def multivariate_energy_distance(X, Y):\n",
        "    X = np.asarray(X); Y = np.asarray(Y)\n",
        "    n = X.shape[0]; m = Y.shape[0]\n",
        "    if n==0 or m==0: return float('nan')\n",
        "    cross = cdist(X, Y, metric=\"euclidean\")\n",
        "    a = (2.0/(n*m))*np.sum(cross)\n",
        "    xx = cdist(X,X,metric=\"euclidean\") if n>1 else np.zeros((1,1))\n",
        "    yy = cdist(Y,Y,metric=\"euclidean\") if m>1 else np.zeros((1,1))\n",
        "    b = (1.0/(n*n))*np.sum(xx) if n>1 else 0.0\n",
        "    c = (1.0/(m*m))*np.sum(yy) if m>1 else 0.0\n",
        "    return float(max(a - b - c, 0.0))\n",
        "\n",
        "files = DEFAULT_MODEL_FILES\n",
        "dfs = {name: load_csv(path) for name, path in zip(MODEL_NAMES, files)}\n",
        "\n",
        "common = set.intersection(*[set(df['Filename'].dropna()) for df in dfs.values()])\n",
        "common = sorted(list(common))\n",
        "aligned = pd.DataFrame({\"Filename\": common})\n",
        "for name, df in dfs.items():\n",
        "    tmp = df.set_index('Filename')\n",
        "    aligned[f\"Caption_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption'] if x in tmp.index else \"\")\n",
        "    aligned[f\"Caption_norm_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption_norm'] if x in tmp.index else \"\")\n",
        "    aligned[f\"first_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'first_num'] if x in tmp.index else None)\n",
        "    aligned[f\"second_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'second_num'] if x in tmp.index else None)\n",
        "\n",
        "def infer_label_from_aligned(aligned, col_prefix, fallback_vals=None):\n",
        "    vals = []\n",
        "    for i in range(len(aligned)):\n",
        "        found = None\n",
        "        for name in MODEL_NAMES:\n",
        "            v = aligned.at[i, f\"{col_prefix}_{name}\"]\n",
        "            if pd.notna(v) and v is not None:\n",
        "                found = v\n",
        "                break\n",
        "        vals.append(found)\n",
        "    return vals\n",
        "\n",
        "first_nums = infer_label_from_aligned(aligned, 'first_num')\n",
        "second_nums = infer_label_from_aligned(aligned, 'second_num')\n",
        "gender_label = []\n",
        "skin_group_label = []\n",
        "for f,s in zip(first_nums, second_nums):\n",
        "    if f in (1,2):\n",
        "        gender_label.append('male' if int(f)==1 else 'female')\n",
        "    else:\n",
        "        gender_label.append('unknown')\n",
        "    skin_group_label.append(secondnum_to_skingroup(s))\n",
        "\n",
        "aligned['gender_label'] = gender_label\n",
        "aligned['skin_group_label'] = skin_group_label\n",
        "\n",
        "model = SentenceTransformer(EMBED_MODEL)\n",
        "dim = model.get_sentence_embedding_dimension()\n",
        "\n",
        "summary = {}\n",
        "for name in MODEL_NAMES:\n",
        "    caps = aligned[f\"Caption_norm_{name}\"].tolist()\n",
        "    embs = model.encode(caps, convert_to_numpy=True, show_progress_bar=True, batch_size=64)\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True); norms[norms==0]=1.0; embs = embs/norms\n",
        "\n",
        "    mask_m = (aligned['gender_label'] == 'male')\n",
        "    mask_f = (aligned['gender_label'] == 'female')\n",
        "    male_cent = embs[mask_m.values].mean(axis=0) if mask_m.sum()>0 else np.zeros((dim,))\n",
        "    female_cent = embs[mask_f.values].mean(axis=0) if mask_f.sum()>0 else np.zeros((dim,))\n",
        "    if np.linalg.norm(male_cent)>0: male_cent = male_cent/np.linalg.norm(male_cent)\n",
        "    if np.linalg.norm(female_cent)>0: female_cent = female_cent/np.linalg.norm(female_cent)\n",
        "    sim_to_male = (embs @ male_cent).astype(float)\n",
        "    sim_to_female = (embs @ female_cent).astype(float)\n",
        "    aligned[f\"sim_to_male_centroid_{name}\"] = sim_to_male\n",
        "    aligned[f\"sim_to_female_centroid_{name}\"] = sim_to_female\n",
        "    gender_ed = multivariate_energy_distance(embs[mask_m.values], embs[mask_f.values]) if (mask_m.sum()>1 and mask_f.sum()>1) else float('nan')\n",
        "\n",
        "    mask_light = (aligned['skin_group_label'] == 'light')\n",
        "    mask_mid = (aligned['skin_group_label'] == 'mid')\n",
        "    mask_dark = (aligned['skin_group_label'] == 'dark')\n",
        "    light_cent = embs[mask_light.values].mean(axis=0) if mask_light.sum()>0 else np.zeros((dim,))\n",
        "    mid_cent   = embs[mask_mid.values].mean(axis=0)   if mask_mid.sum()>0   else np.zeros((dim,))\n",
        "    dark_cent  = embs[mask_dark.values].mean(axis=0)  if mask_dark.sum()>0  else np.zeros((dim,))\n",
        "    if np.linalg.norm(light_cent)>0: light_cent = light_cent/np.linalg.norm(light_cent)\n",
        "    if np.linalg.norm(mid_cent)>0:   mid_cent = mid_cent/np.linalg.norm(mid_cent)\n",
        "    if np.linalg.norm(dark_cent)>0:  dark_cent = dark_cent/np.linalg.norm(dark_cent)\n",
        "\n",
        "    sim_to_light = (embs @ light_cent).astype(float)\n",
        "    sim_to_mid   = (embs @ mid_cent).astype(float)\n",
        "    sim_to_dark  = (embs @ dark_cent).astype(float)\n",
        "    aligned[f\"sim_to_skin_light_centroid_{name}\"] = sim_to_light\n",
        "    aligned[f\"sim_to_skin_mid_centroid_{name}\"] = sim_to_mid\n",
        "    aligned[f\"sim_to_skin_dark_centroid_{name}\"] = sim_to_dark\n",
        "\n",
        "    ed_light_mid = multivariate_energy_distance(embs[mask_light.values], embs[mask_mid.values]) if (mask_light.sum()>1 and mask_mid.sum()>1) else float('nan')\n",
        "    ed_light_dark = multivariate_energy_distance(embs[mask_light.values], embs[mask_dark.values]) if (mask_light.sum()>1 and mask_dark.sum()>1) else float('nan')\n",
        "    ed_mid_dark   = multivariate_energy_distance(embs[mask_mid.values],   embs[mask_dark.values]) if (mask_mid.sum()>1 and mask_dark.sum()>1) else float('nan')\n",
        "\n",
        "    summary[name] = {\n",
        "        \"gender\": {\"energy_distance\": float(gender_ed), \"n_m\": int(mask_m.sum()), \"n_f\": int(mask_f.sum())},\n",
        "        \"skin_groups\": {\n",
        "            \"n_light\": int(mask_light.sum()), \"n_mid\": int(mask_mid.sum()), \"n_dark\": int(mask_dark.sum()),\n",
        "            \"energy_distance_light_mid\": float(ed_light_mid),\n",
        "            \"energy_distance_light_dark\": float(ed_light_dark),\n",
        "            \"energy_distance_mid_dark\": float(ed_mid_dark)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"Filename\": aligned['Filename'],\n",
        "        \"Caption\": aligned[f\"Caption_norm_{name}\"],\n",
        "        \"sim_to_male_centroid\": aligned[f\"sim_to_male_centroid_{name}\"],\n",
        "        \"sim_to_female_centroid\": aligned[f\"sim_to_female_centroid_{name}\"],\n",
        "        \"sim_to_skin_light_centroid\": aligned[f\"sim_to_skin_light_centroid_{name}\"],\n",
        "        \"sim_to_skin_mid_centroid\": aligned[f\"sim_to_skin_mid_centroid_{name}\"],\n",
        "        \"sim_to_skin_dark_centroid\": aligned[f\"sim_to_skin_dark_centroid_{name}\"],\n",
        "        \"gender_label\": aligned['gender_label'],\n",
        "        \"skin_group_label\": aligned['skin_group_label']\n",
        "    })\n",
        "    out_path = os.path.join(OUT_DIR, f\"part1_bias_metrics_{name}.csv\")\n",
        "    out.to_csv(out_path, index=False)\n",
        "    print(\"Saved Part1 CSV:\", out_path)\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"part1_summary.json\"), \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump({\"timestamp\": datetime.now(timezone.utc).isoformat(), \"summary\": summary}, jf, indent=2)\n",
        "print(\"Part1 done. Summary saved to part1_summary.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyIH-NbOmLYp"
      },
      "source": [
        "2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub2-SjUGmMJb"
      },
      "source": [
        "3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5cvcnUdRqMU"
      },
      "outputs": [],
      "source": [
        "import os, json, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "from collections import defaultdict\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/image_captioning_bias\"\n",
        "OUT_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "DEFAULT_MODEL_FILES = [\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/chatgpt/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/claude/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/gemini/captions.csv\"\n",
        "]\n",
        "MODEL_NAMES = [\"chatgpt\",\"claude\",\"gemini\"]\n",
        "\n",
        "GENDER_TOKEN_BASE = [\"man\",\"woman\",\"male\",\"female\",\"boy\",\"girl\",\"men\",\"women\",\"his\",\"her\",\"he\",\"she\",\"him\"]\n",
        "GENDER_TOKENS = set(GENDER_TOKEN_BASE + [w.capitalize() for w in GENDER_TOKEN_BASE])\n",
        "RACE_TOKENS = {\"Black\",\"White\",\"Asian\",\"Brown\",\"Latino\",\"Hispanic\",\"Indian\",\"African\",\"Caucasian\",\"Middle-Eastern\",\"Arab\"}\n",
        "\n",
        "SKIN_TO_GROUP = {1: \"light\", 2: \"light\", 3: \"mid\", 4: \"mid\", 5: \"dark\", 6: \"dark\"}\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\"]\n",
        "    text_col = None\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            text_col = c; break\n",
        "    if text_col is None:\n",
        "        string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "        if not string_cols:\n",
        "            raise KeyError(f\"No text-like column found in {path}\")\n",
        "        text_col = max(string_cols, key=lambda c: df[c].astype(str).map(len).mean())\n",
        "    if 'Filename' not in df.columns and 'filename' in df.columns:\n",
        "        df.rename(columns={'filename':'Filename'}, inplace=True)\n",
        "    df = df.copy(); df.rename(columns={text_col:\"Caption\"}, inplace=True)\n",
        "    df['Caption_norm'] = df['Caption'].astype(str).str.strip()\n",
        "    def extract_numbers(fname):\n",
        "        try:\n",
        "            p=str(fname).split('_'); return int(p[0]), int(p[1])\n",
        "        except: return None, None\n",
        "    if 'Filename' in df.columns:\n",
        "        nums = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "        df[['first_num','second_num']] = nums\n",
        "    return df\n",
        "\n",
        "dfs = {name: load_csv(p) for name,p in zip(MODEL_NAMES, DEFAULT_MODEL_FILES)}\n",
        "common = set.intersection(*[set(df['Filename'].dropna()) for df in dfs.values()])\n",
        "common = sorted(list(common))\n",
        "aligned = pd.DataFrame({\"Filename\": common})\n",
        "for name, df in dfs.items():\n",
        "    tmp = df.set_index('Filename')\n",
        "    aligned[f\"Caption_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption'] if x in tmp.index else \"\")\n",
        "    aligned[f\"Caption_norm_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption_norm'] if x in tmp.index else \"\")\n",
        "    aligned[f\"first_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'first_num'] if x in tmp.index else None)\n",
        "    aligned[f\"second_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'second_num'] if x in tmp.index else None)\n",
        "\n",
        "def simple_tokenize_preserve_case(text):\n",
        "    txt = str(text)\n",
        "    txt = re.sub(r\"[^\\w\\s'-]\",\" \", txt)\n",
        "    toks = [t for t in txt.split() if t]\n",
        "    return toks\n",
        "\n",
        "part3_summary_extended = {\"by_model\": {}, \"skin_pairwise\": {}}\n",
        "\n",
        "for name in MODEL_NAMES:\n",
        "    caps = aligned[f\"Caption_norm_{name}\"].tolist()\n",
        "    gender_flags = []\n",
        "    race_flags = []\n",
        "    for cap in caps:\n",
        "        toks = simple_tokenize_preserve_case(cap)\n",
        "        gender_mention = any((t in GENDER_TOKENS) or (t.lower() in GENDER_TOKEN_BASE) for t in toks)\n",
        "        race_mention = any(t in RACE_TOKENS for t in toks)\n",
        "        gender_flags.append(int(gender_mention))\n",
        "        race_flags.append(int(race_mention))\n",
        "\n",
        "    aligned[f\"gender_mention_{name}\"] = gender_flags\n",
        "    aligned[f\"race_mention_{name}\"] = race_flags\n",
        "\n",
        "    genders = []\n",
        "    skin_groups = []\n",
        "    for i in range(len(aligned)):\n",
        "        v = aligned.at[i, f\"first_num_{name}\"]\n",
        "        if pd.isna(v) or v is None:\n",
        "            genders.append(\"unknown\")\n",
        "        else:\n",
        "            genders.append(\"male\" if int(v)==1 else \"female\")\n",
        "        s = aligned.at[i, f\"second_num_{name}\"]\n",
        "        if pd.isna(s) or s is None:\n",
        "            skin_groups.append(\"unknown\")\n",
        "        else:\n",
        "            try:\n",
        "                sg = SKIN_TO_GROUP.get(int(s), \"unknown\")\n",
        "            except Exception:\n",
        "                sg = \"unknown\"\n",
        "            skin_groups.append(sg)\n",
        "\n",
        "    genders = np.array(genders)\n",
        "    skin_groups = np.array(skin_groups)\n",
        "\n",
        "    male_mask = genders == 'male'\n",
        "    female_mask = genders == 'female'\n",
        "    overall_gender_mention_rate = float(np.mean(gender_flags)) if len(gender_flags)>0 else float('nan')\n",
        "    male_gender_mention_rate = float(np.mean(np.array(gender_flags)[male_mask])) if male_mask.sum()>0 else float('nan')\n",
        "    female_gender_mention_rate = float(np.mean(np.array(gender_flags)[female_mask])) if female_mask.sum()>0 else float('nan')\n",
        "\n",
        "    overall_race_mention_rate = float(np.mean(race_flags)) if len(race_flags)>0 else float('nan')\n",
        "    male_race_mention_rate = float(np.mean(np.array(race_flags)[male_mask])) if male_mask.sum()>0 else float('nan')\n",
        "    female_race_mention_rate = float(np.mean(np.array(race_flags)[female_mask])) if female_mask.sum()>0 else float('nan')\n",
        "\n",
        "    skin_rates = {}\n",
        "    for sg in [\"light\",\"mid\",\"dark\",\"unknown\"]:\n",
        "        mask = skin_groups == sg\n",
        "        if mask.sum() > 0:\n",
        "            skin_rates[f\"{sg}_gender_mention_rate\"] = float(np.mean(np.array(gender_flags)[mask]))\n",
        "            skin_rates[f\"{sg}_race_mention_rate\"] = float(np.mean(np.array(race_flags)[mask]))\n",
        "            skin_rates[f\"{sg}_n\"] = int(mask.sum())\n",
        "        else:\n",
        "            skin_rates[f\"{sg}_gender_mention_rate\"] = float('nan')\n",
        "            skin_rates[f\"{sg}_race_mention_rate\"] = float('nan')\n",
        "            skin_rates[f\"{sg}_n\"] = 0\n",
        "\n",
        "    part3_summary_extended[\"by_model\"][name] = {\n",
        "        \"overall_gender_mention_rate\": overall_gender_mention_rate,\n",
        "        \"male_gender_mention_rate\": male_gender_mention_rate,\n",
        "        \"female_gender_mention_rate\": female_gender_mention_rate,\n",
        "        \"overall_race_mention_rate\": overall_race_mention_rate,\n",
        "        \"male_race_mention_rate\": male_race_mention_rate,\n",
        "        \"female_race_mention_rate\": female_race_mention_rate,\n",
        "        \"skin_group_rates\": skin_rates\n",
        "    }\n",
        "\n",
        "    out_df = pd.DataFrame({\n",
        "        \"Filename\": aligned['Filename'],\n",
        "        \"Caption\": aligned[f\"Caption_norm_{name}\"],\n",
        "        \"inferred_gender\": genders,\n",
        "        \"inferred_skin\": skin_groups,\n",
        "        \"gender_mention\": aligned[f\"gender_mention_{name}\"],\n",
        "        \"race_mention\": aligned[f\"race_mention_{name}\"]\n",
        "    })\n",
        "    out_path = os.path.join(OUT_DIR, f\"part3_bias_metrics_{name}.csv\")\n",
        "    out_df.to_csv(out_path, index=False)\n",
        "    print(\"Saved extended Part3 CSV for model:\", name, \"->\", out_path)\n",
        "\n",
        "for name in MODEL_NAMES:\n",
        "    skin_metrics = {}\n",
        "    for a, b in [(\"light\",\"mid\"), (\"light\",\"dark\"), (\"mid\",\"dark\")]:\n",
        "        a_rate = part3_summary_extended[\"by_model\"][name][\"skin_group_rates\"].get(f\"{a}_gender_mention_rate\", float('nan'))\n",
        "        b_rate = part3_summary_extended[\"by_model\"][name][\"skin_group_rates\"].get(f\"{b}_gender_mention_rate\", float('nan'))\n",
        "        if np.isfinite(a_rate) and np.isfinite(b_rate):\n",
        "            skin_metrics[f\"{a}_vs_{b}_gender_mention_diff\"] = abs(a_rate - b_rate)\n",
        "        else:\n",
        "            skin_metrics[f\"{a}_vs_{b}_gender_mention_diff\"] = float('nan')\n",
        "\n",
        "        a_r = part3_summary_extended[\"by_model\"][name][\"skin_group_rates\"].get(f\"{a}_race_mention_rate\", float('nan'))\n",
        "        b_r = part3_summary_extended[\"by_model\"][name][\"skin_group_rates\"].get(f\"{b}_race_mention_rate\", float('nan'))\n",
        "        if np.isfinite(a_r) and np.isfinite(b_r):\n",
        "            skin_metrics[f\"{a}_vs_{b}_race_mention_diff\"] = abs(a_r - b_r)\n",
        "        else:\n",
        "            skin_metrics[f\"{a}_vs_{b}_race_mention_diff\"] = float('nan')\n",
        "\n",
        "    part3_summary_extended[\"skin_pairwise\"][name] = skin_metrics\n",
        "\n",
        "summary_path = os.path.join(OUT_DIR, \"part3_summary_extended.json\")\n",
        "with open(summary_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump({\"timestamp\": datetime.now(timezone.utc).isoformat(), \"summary\": part3_summary_extended}, jf, indent=2)\n",
        "print(\"Saved extended Part3 summary JSON ->\", summary_path)\n",
        "print(\"Part3 (extended) done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYBqV7_SmM9g"
      },
      "source": [
        "4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVew-tG8R98Q"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/image_captioning_bias\"\n",
        "OUT_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "DEFAULT_MODEL_FILES = [\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/chatgpt/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/claude/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/gemini/captions.csv\"\n",
        "]\n",
        "MODEL_NAMES = [\"chatgpt\",\"claude\",\"gemini\"]\n",
        "\n",
        "SKIN_TO_GROUP = {1: \"light\", 2: \"light\", 3: \"mid\", 4: \"mid\", 5: \"dark\", 6: \"dark\"}\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\"]\n",
        "    text_col = None\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            text_col = c; break\n",
        "    if text_col is None:\n",
        "        string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "        if not string_cols:\n",
        "            raise KeyError(f\"No text-like column found in {path}\")\n",
        "        text_col = max(string_cols, key=lambda c: df[c].astype(str).map(len).mean())\n",
        "    if 'Filename' not in df.columns and 'filename' in df.columns:\n",
        "        df.rename(columns={'filename':'Filename'}, inplace=True)\n",
        "    df = df.copy(); df.rename(columns={text_col:\"Caption\"}, inplace=True)\n",
        "    df['Caption_norm'] = df['Caption'].astype(str).str.strip()\n",
        "    def extract_numbers(fname):\n",
        "        try:\n",
        "            p=str(fname).split('_'); return int(p[0]), int(p[1])\n",
        "        except: return None, None\n",
        "    if 'Filename' in df.columns:\n",
        "        nums = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "        df[['first_num','second_num']] = nums\n",
        "    return df\n",
        "\n",
        "dfs = {name: load_csv(p) for name,p in zip(MODEL_NAMES, DEFAULT_MODEL_FILES)}\n",
        "common = set.intersection(*[set(df['Filename'].dropna()) for df in dfs.values()])\n",
        "common = sorted(list(common))\n",
        "aligned = pd.DataFrame({\"Filename\": common})\n",
        "for name, df in dfs.items():\n",
        "    tmp = df.set_index('Filename')\n",
        "    aligned[f\"Caption_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption'] if x in tmp.index else \"\")\n",
        "    aligned[f\"Caption_norm_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption_norm'] if x in tmp.index else \"\")\n",
        "    aligned[f\"first_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'first_num'] if x in tmp.index else None)\n",
        "    aligned[f\"second_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'second_num'] if x in tmp.index else None)\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "part4_summary_extended = {\"by_model\": {}, \"skin_pairwise\": {}}\n",
        "\n",
        "for name in MODEL_NAMES:\n",
        "    caps = aligned[f\"Caption_norm_{name}\"].tolist()\n",
        "    comp_scores = [sia.polarity_scores(str(c))['compound'] for c in caps]\n",
        "    aligned[f\"sentiment_compound_{name}\"] = comp_scores\n",
        "\n",
        "    genders = []\n",
        "    skin_groups = []\n",
        "    for i in range(len(aligned)):\n",
        "        v = aligned.at[i, f\"first_num_{name}\"]\n",
        "        if pd.isna(v) or v is None:\n",
        "            genders.append(\"unknown\")\n",
        "        else:\n",
        "            try:\n",
        "                genders.append(\"male\" if int(v)==1 else \"female\")\n",
        "            except Exception:\n",
        "                genders.append(\"unknown\")\n",
        "        s = aligned.at[i, f\"second_num_{name}\"]\n",
        "        if pd.isna(s) or s is None:\n",
        "            skin_groups.append(\"unknown\")\n",
        "        else:\n",
        "            try:\n",
        "                skin_groups.append(SKIN_TO_GROUP.get(int(s), \"unknown\"))\n",
        "            except Exception:\n",
        "                skin_groups.append(\"unknown\")\n",
        "    genders = np.array(genders)\n",
        "    skin_groups = np.array(skin_groups)\n",
        "\n",
        "    male_mask = genders == 'male'\n",
        "    female_mask = genders == 'female'\n",
        "    male_mean = float(np.nanmean(np.array(comp_scores)[male_mask])) if male_mask.sum()>0 else float('nan')\n",
        "    female_mean = float(np.nanmean(np.array(comp_scores)[female_mask])) if female_mask.sum()>0 else float('nan')\n",
        "    abs_diff_gender = float(np.nan if (np.isnan(male_mean) or np.isnan(female_mean)) else abs(male_mean - female_mean))\n",
        "\n",
        "    skin_rates = {}\n",
        "    for sg in [\"light\",\"mid\",\"dark\",\"unknown\"]:\n",
        "        mask = skin_groups == sg\n",
        "        if mask.sum() > 0:\n",
        "            mean_sg = float(np.nanmean(np.array(comp_scores)[mask]))\n",
        "            skin_rates[f\"{sg}_sentiment_mean\"] = mean_sg\n",
        "            skin_rates[f\"{sg}_n\"] = int(mask.sum())\n",
        "        else:\n",
        "            skin_rates[f\"{sg}_sentiment_mean\"] = float('nan')\n",
        "            skin_rates[f\"{sg}_n\"] = 0\n",
        "\n",
        "    part4_summary_extended[\"by_model\"][name] = {\n",
        "        \"male_sentiment_mean\": male_mean,\n",
        "        \"female_sentiment_mean\": female_mean,\n",
        "        \"abs_diff_gender\": abs_diff_gender,\n",
        "        \"skin_group_sentiment_means\": skin_rates\n",
        "    }\n",
        "\n",
        "    out_df = pd.DataFrame({\n",
        "        \"Filename\": aligned['Filename'],\n",
        "        \"Caption\": aligned[f\"Caption_norm_{name}\"],\n",
        "        \"inferred_gender\": genders,\n",
        "        \"inferred_skin\": skin_groups,\n",
        "        \"sentiment_compound\": comp_scores\n",
        "    })\n",
        "    out_path = os.path.join(OUT_DIR, f\"part4_bias_metrics_{name}.csv\")\n",
        "    out_df.to_csv(out_path, index=False)\n",
        "    print(\"Saved extended Part4 CSV for model:\", name, \"->\", out_path)\n",
        "\n",
        "    skin_pairwise = {}\n",
        "    def safe_mean(sg):\n",
        "        v = part4_summary_extended[\"by_model\"][name][\"skin_group_sentiment_means\"].get(f\"{sg}_sentiment_mean\")\n",
        "        return v\n",
        "    for a,b in [(\"light\",\"mid\"), (\"light\",\"dark\"), (\"mid\",\"dark\")]:\n",
        "        ma = safe_mean(a); mb = safe_mean(b)\n",
        "        if not (np.isnan(ma) or np.isnan(mb)):\n",
        "            skin_pairwise[f\"{a}_vs_{b}_abs_diff\"] = abs(ma - mb)\n",
        "        else:\n",
        "            skin_pairwise[f\"{a}_vs_{b}_abs_diff\"] = float('nan')\n",
        "    part4_summary_extended[\"skin_pairwise\"][name] = skin_pairwise\n",
        "\n",
        "summary_path = os.path.join(OUT_DIR, \"part4_summary_extended.json\")\n",
        "with open(summary_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump({\"timestamp\": datetime.now(timezone.utc).isoformat(), \"summary\": part4_summary_extended}, jf, indent=2)\n",
        "\n",
        "print(\"Saved extended Part4 summary JSON ->\", summary_path)\n",
        "print(\"Extended Part4 done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBCGGFo4mNnb"
      },
      "source": [
        "5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_pqTd0kSzqY"
      },
      "outputs": [],
      "source": [
        "import os, re, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/image_captioning_bias\"\n",
        "OUT_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "DEFAULT_MODEL_FILES = [\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/chatgpt/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/claude/captions.csv\",\n",
        "    \"/content/drive/MyDrive/image_captioning_bias/gemini/captions.csv\"\n",
        "]\n",
        "MODEL_NAMES = [\"chatgpt\",\"claude\",\"gemini\"]\n",
        "\n",
        "SKIN_TO_GROUP = {1: \"light\", 2: \"light\", 3: \"mid\", 4: \"mid\", 5: \"dark\", 6: \"dark\"}\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    candidates = [\"caption\",\"Caption\",\"captions\",\"text\",\"Text\",\"caption_text\",\"description\"]\n",
        "    text_col = None\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            text_col = c; break\n",
        "    if text_col is None:\n",
        "        string_cols = [col for col in df.columns if df[col].dtype == object or pd.api.types.is_string_dtype(df[col])]\n",
        "        if not string_cols:\n",
        "            raise KeyError(f\"No text-like column found in {path}\")\n",
        "        text_col = max(string_cols, key=lambda c: df[c].astype(str).map(len).mean())\n",
        "    if 'Filename' not in df.columns and 'filename' in df.columns:\n",
        "        df.rename(columns={'filename':'Filename'}, inplace=True)\n",
        "    df = df.copy(); df.rename(columns={text_col:\"Caption\"}, inplace=True)\n",
        "    df['Caption_norm'] = df['Caption'].astype(str).str.strip()\n",
        "    def extract_numbers(fname):\n",
        "        try:\n",
        "            p=str(fname).split('_'); return int(p[0]), int(p[1])\n",
        "        except: return None, None\n",
        "    if 'Filename' in df.columns:\n",
        "        nums = df['Filename'].apply(lambda x: pd.Series(extract_numbers(x)))\n",
        "        df[['first_num','second_num']] = nums\n",
        "    return df\n",
        "\n",
        "dfs = {name: load_csv(p) for name,p in zip(MODEL_NAMES, DEFAULT_MODEL_FILES)}\n",
        "common = set.intersection(*[set(df['Filename'].dropna()) for df in dfs.values()])\n",
        "common = sorted(list(common))\n",
        "aligned = pd.DataFrame({\"Filename\": common})\n",
        "for name, df in dfs.items():\n",
        "    tmp = df.set_index('Filename')\n",
        "    aligned[f\"Caption_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption'] if x in tmp.index else \"\")\n",
        "    aligned[f\"Caption_norm_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'Caption_norm'] if x in tmp.index else \"\")\n",
        "    aligned[f\"first_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'first_num'] if x in tmp.index else None)\n",
        "    aligned[f\"second_num_{name}\"] = aligned['Filename'].map(lambda x: tmp.loc[x,'second_num'] if x in tmp.index else None)\n",
        "\n",
        "def tokenize_preserve_case(text):\n",
        "    txt = str(text)\n",
        "    txt = re.sub(r\"[^\\w\\s'-]\",\" \", txt)\n",
        "    toks = [t for t in txt.split() if t]\n",
        "    return toks\n",
        "\n",
        "def ttr(text):\n",
        "    toks = tokenize_preserve_case(text)\n",
        "    if len(toks)==0: return 0.0\n",
        "    return len(set([t.lower() for t in toks])) / float(len(toks))\n",
        "\n",
        "part5_summary_extended = {\"by_model\": {}, \"skin_pairwise\": {}}\n",
        "\n",
        "for name in MODEL_NAMES:\n",
        "    caps = aligned[f\"Caption_norm_{name}\"].tolist()\n",
        "    ttrs = [ttr(c) for c in caps]\n",
        "    aligned[f\"ttr_{name}\"] = ttrs\n",
        "\n",
        "    genders = []\n",
        "    skin_groups = []\n",
        "    for i in range(len(aligned)):\n",
        "        v = aligned.at[i, f\"first_num_{name}\"]\n",
        "        if pd.isna(v) or v is None:\n",
        "            genders.append(\"unknown\")\n",
        "        else:\n",
        "            try:\n",
        "                genders.append(\"male\" if int(v)==1 else \"female\")\n",
        "            except Exception:\n",
        "                genders.append(\"unknown\")\n",
        "        s = aligned.at[i, f\"second_num_{name}\"]\n",
        "        if pd.isna(s) or s is None:\n",
        "            skin_groups.append(\"unknown\")\n",
        "        else:\n",
        "            try:\n",
        "                skin_groups.append(SKIN_TO_GROUP.get(int(s), \"unknown\"))\n",
        "            except Exception:\n",
        "                skin_groups.append(\"unknown\")\n",
        "    genders = np.array(genders)\n",
        "    skin_groups = np.array(skin_groups)\n",
        "\n",
        "    male_mask = genders == 'male'\n",
        "    female_mask = genders == 'female'\n",
        "    male_mean = float(np.nanmean(np.array(ttrs)[male_mask])) if male_mask.sum()>0 else float('nan')\n",
        "    female_mean = float(np.nanmean(np.array(ttrs)[female_mask])) if female_mask.sum()>0 else float('nan')\n",
        "    abs_diff_gender = float(np.nan if (np.isnan(male_mean) or np.isnan(female_mean)) else abs(male_mean - female_mean))\n",
        "\n",
        "    skin_rates = {}\n",
        "    for sg in [\"light\",\"mid\",\"dark\",\"unknown\"]:\n",
        "        mask = skin_groups == sg\n",
        "        if mask.sum() > 0:\n",
        "            mean_sg = float(np.nanmean(np.array(ttrs)[mask]))\n",
        "            skin_rates[f\"{sg}_ttr_mean\"] = mean_sg\n",
        "            skin_rates[f\"{sg}_n\"] = int(mask.sum())\n",
        "        else:\n",
        "            skin_rates[f\"{sg}_ttr_mean\"] = float('nan')\n",
        "            skin_rates[f\"{sg}_n\"] = 0\n",
        "\n",
        "    part5_summary_extended[\"by_model\"][name] = {\n",
        "        \"male_ttr_mean\": male_mean,\n",
        "        \"female_ttr_mean\": female_mean,\n",
        "        \"abs_diff_gender\": abs_diff_gender,\n",
        "        \"skin_group_ttr_means\": skin_rates\n",
        "    }\n",
        "\n",
        "    out_df = pd.DataFrame({\n",
        "        \"Filename\": aligned['Filename'],\n",
        "        \"Caption\": aligned[f\"Caption_norm_{name}\"],\n",
        "        \"inferred_gender\": genders,\n",
        "        \"inferred_skin\": skin_groups,\n",
        "        \"ttr\": ttrs\n",
        "    })\n",
        "    out_path = os.path.join(OUT_DIR, f\"part5_bias_metrics_{name}.csv\")\n",
        "    out_df.to_csv(out_path, index=False)\n",
        "    print(\"Saved extended Part5 CSV for model:\", name, \"->\", out_path)\n",
        "\n",
        "    skin_pairwise = {}\n",
        "    def safe_mean(sg):\n",
        "        return part5_summary_extended[\"by_model\"][name][\"skin_group_ttr_means\"].get(f\"{sg}_ttr_mean\")\n",
        "    for a,b in [(\"light\",\"mid\"), (\"light\",\"dark\"), (\"mid\",\"dark\")]:\n",
        "        ma = safe_mean(a); mb = safe_mean(b)\n",
        "        if not (np.isnan(ma) or np.isnan(mb)):\n",
        "            skin_pairwise[f\"{a}_vs_{b}_abs_diff\"] = abs(ma - mb)\n",
        "        else:\n",
        "            skin_pairwise[f\"{a}_vs_{b}_abs_diff\"] = float('nan')\n",
        "    part5_summary_extended[\"skin_pairwise\"][name] = skin_pairwise\n",
        "\n",
        "summary_path = os.path.join(OUT_DIR, \"part5_summary_extended.json\")\n",
        "with open(summary_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump({\"timestamp\": datetime.now(timezone.utc).isoformat(), \"summary\": part5_summary_extended}, jf, indent=2)\n",
        "print(\"Saved extended Part5 summary JSON ->\", summary_path)\n",
        "print(\"Extended Part5 done.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T5TM6L_iJd4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Es4Rq7rmLOI"
      },
      "outputs": [],
      "source": [
        "import os, json, math\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import cdist\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/image_captioning_bias\"\n",
        "IN_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs\")\n",
        "OUT_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs_part7\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAMES = [\"chatgpt\",\"claude\",\"gemini\"]\n",
        "N_PERMS = 1000\n",
        "RNG_SEED = 42\n",
        "\n",
        "PART1_CSV = os.path.join(IN_DIR, \"part1_bias_metrics_{m}.csv\")\n",
        "PART2_CSV = os.path.join(IN_DIR, \"part2_bias_metrics_{m}.csv\")\n",
        "PART3_CSV = os.path.join(IN_DIR, \"part3_bias_metrics_{m}.csv\")\n",
        "PART4_CSV = os.path.join(IN_DIR, \"part4_bias_metrics_{m}.csv\")\n",
        "PART5_CSV = os.path.join(IN_DIR, \"part5_bias_metrics_{m}.csv\")\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not read {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def multivariate_energy_distance(X, Y):\n",
        "    X = np.asarray(X); Y = np.asarray(Y)\n",
        "    if X.ndim != 2 or Y.ndim != 2:\n",
        "        raise ValueError(\"X and Y must be 2D arrays.\")\n",
        "    n = X.shape[0]; m = Y.shape[0]\n",
        "    if n == 0 or m == 0:\n",
        "        return float('nan')\n",
        "    cross = cdist(X, Y, metric=\"euclidean\")\n",
        "    a = (2.0/(n*m))*np.sum(cross)\n",
        "    xx = cdist(X, X, metric=\"euclidean\") if n>1 else np.zeros((1,1))\n",
        "    yy = cdist(Y, Y, metric=\"euclidean\") if m>1 else np.zeros((1,1))\n",
        "    b = (1.0/(n*n))*np.sum(xx) if n>1 else 0.0\n",
        "    c = (1.0/(m*m))*np.sum(yy) if m>1 else 0.0\n",
        "    return float(max(a - b - c, 0.0))\n",
        "\n",
        "def two_prop_ztest(x1,n1,x2,n2):\n",
        "    if n1==0 or n2==0:\n",
        "        return float('nan'), float('nan')\n",
        "    p1 = x1 / n1\n",
        "    p2 = x2 / n2\n",
        "    p_pool = (x1 + x2) / (n1 + n2)\n",
        "    denom = math.sqrt(p_pool*(1-p_pool)*(1.0/n1 + 1.0/n2))\n",
        "    if denom == 0:\n",
        "        return float('nan'), float('nan')\n",
        "    z = (p1 - p2) / denom\n",
        "    p = 2.0 * (1.0 - stats.norm.cdf(abs(z)))\n",
        "    return float(z), float(p)\n",
        "\n",
        "def cohens_d_from_arrays(a,b):\n",
        "    a = np.asarray(a); b = np.asarray(b)\n",
        "    if len(a)<2 or len(b)<2:\n",
        "        return float('nan')\n",
        "    ma, mb = np.nanmean(a), np.nanmean(b)\n",
        "    sa, sb = np.nanstd(a, ddof=1), np.nanstd(b, ddof=1)\n",
        "    pooled = math.sqrt(((len(a)-1)*sa*sa + (len(b)-1)*sb*sb) / (len(a)+len(b)-2))\n",
        "    if pooled == 0:\n",
        "        return float('nan')\n",
        "    return float((ma-mb)/pooled)\n",
        "\n",
        "def compute_chi2_across_models_binary(arrs):\n",
        "    table = []\n",
        "    total = 0\n",
        "    for a in arrs:\n",
        "        a = np.asarray(a)\n",
        "        a = a[~np.isnan(a)]\n",
        "        n = len(a)\n",
        "        x = int(np.sum(a==1)) if n>0 else 0\n",
        "        table.append([n-x, x])\n",
        "        total += n\n",
        "    if total == 0:\n",
        "        return None, None, table\n",
        "    try:\n",
        "        chi2, p, _, _ = stats.chi2_contingency(np.array(table))\n",
        "        return float(chi2), float(p), table\n",
        "    except Exception:\n",
        "        return None, None, table\n",
        "\n",
        "def compute_anova_across_models(arrs):\n",
        "    groups = [a[~np.isnan(a)] for a in arrs if len(a[~np.isnan(a)])>0]\n",
        "    if len(groups) < 2:\n",
        "        return None, None\n",
        "    try:\n",
        "        f, p = stats.f_oneway(*groups)\n",
        "        return float(f), float(p)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "print(\"Loading Part1..Part5 CSVs for each model from:\", IN_DIR)\n",
        "csvs = {}\n",
        "for m in MODEL_NAMES:\n",
        "    csvs[m] = {\n",
        "        \"part1\": safe_read_csv(PART1_CSV.format(m=m)),\n",
        "        \"part2\": safe_read_csv(PART2_CSV.format(m=m)),\n",
        "        \"part3\": safe_read_csv(PART3_CSV.format(m=m)),\n",
        "        \"part4\": safe_read_csv(PART4_CSV.format(m=m)),\n",
        "        \"part5\": safe_read_csv(PART5_CSV.format(m=m)),\n",
        "    }\n",
        "    counts = {k: (len(v) if v is not None else 0) for k,v in csvs[m].items()}\n",
        "    print(f\"  {m}: rows per part:\", counts)\n",
        "\n",
        "results = {\"timestamp\": datetime.now(timezone.utc).isoformat(), \"models\": {}}\n",
        "for m in MODEL_NAMES:\n",
        "    print(f\"\\nRecomputing stats for model: {m}\")\n",
        "    part1_df = csvs[m][\"part1\"]\n",
        "    part2_df = csvs[m][\"part2\"]\n",
        "    part3_df = csvs[m][\"part3\"]\n",
        "    part4_df = csvs[m][\"part4\"]\n",
        "    part5_df = csvs[m][\"part5\"]\n",
        "\n",
        "    model_out = {\"part1\":{}, \"part2\":{}, \"part3\":{}, \"part4\":{}, \"part5\":{}}\n",
        "\n",
        "    if part1_df is None:\n",
        "        print(\"  WARNING: part1 CSV missing for\", m)\n",
        "        model_out['part1'] = {\"error\":\"part1_csv_missing\"}\n",
        "    else:\n",
        "        def pick(colnames):\n",
        "            for c in colnames:\n",
        "                if c in part1_df.columns:\n",
        "                    return c\n",
        "            return None\n",
        "        c_sim_male = pick([\"sim_to_male_centroid\",\"sim_to_male_centroid_\"+m,\"sim_to_male_centroid\"])\n",
        "        c_sim_fem  = pick([\"sim_to_female_centroid\",\"sim_to_female_centroid_\"+m,\"sim_to_female_centroid\"])\n",
        "        c_sim_light= pick([\"sim_to_skin_light_centroid\",\"sim_to_skin_light_centroid_\"+m,\"sim_to_skin_light_centroid\"])\n",
        "        c_sim_mid  = pick([\"sim_to_skin_mid_centroid\",\"sim_to_skin_mid_centroid_\"+m,\"sim_to_skin_mid_centroid\"])\n",
        "        c_sim_dark = pick([\"sim_to_skin_dark_centroid\",\"sim_to_skin_dark_centroid_\"+m,\"sim_to_skin_dark_centroid\"])\n",
        "        feature_cols = [c for c in [c_sim_male, c_sim_fem, c_sim_light, c_sim_mid, c_sim_dark] if c is not None]\n",
        "        if not feature_cols:\n",
        "            print(\"  ERROR: no sim columns found in part1 CSV for\", m)\n",
        "            model_out['part1'] = {\"error\":\"no_sim_columns\"}\n",
        "        else:\n",
        "            feat = part1_df[feature_cols].apply(pd.to_numeric, errors='coerce').to_numpy(dtype=float)\n",
        "            gender_col = None\n",
        "            for cand in [\"gender_label\",\"inferred_gender\",\"inferred_gender_\"+m,\"gender\"]:\n",
        "                if cand in part1_df.columns:\n",
        "                    gender_col = cand; break\n",
        "            skin_col = None\n",
        "            for cand in [\"skin_group_label\",\"inferred_skin\",\"inferred_skin_\"+m,\"skin_group\"]:\n",
        "                if cand in part1_df.columns:\n",
        "                    skin_col = cand; break\n",
        "\n",
        "            genders = part1_df[gender_col].astype(str).values if gender_col is not None else np.array([\"unknown\"]*len(part1_df))\n",
        "            skins   = part1_df[skin_col].astype(str).values if skin_col is not None else np.array([\"unknown\"]*len(part1_df))\n",
        "\n",
        "            mask_m = (genders == \"male\"); mask_f = (genders == \"female\")\n",
        "            X = feat[mask_m]; Y = feat[mask_f]\n",
        "            ed_obs = multivariate_energy_distance(X, Y) if (X.shape[0]>0 and Y.shape[0]>0 and X.ndim==2 and Y.ndim==2) else float('nan')\n",
        "            perm_count = 0\n",
        "            all_idx = np.arange(len(feat))\n",
        "            n_m = int(mask_m.sum()); n_f = int(mask_f.sum())\n",
        "            if n_m>0 and n_f>0:\n",
        "                for i in range(N_PERMS):\n",
        "                    perm = rng.permutation(all_idx)\n",
        "                    a_idx = perm[:n_m]; b_idx = perm[n_m:n_m+n_f]\n",
        "                    val = multivariate_energy_distance(feat[a_idx], feat[b_idx])\n",
        "                    if val >= ed_obs:\n",
        "                        perm_count += 1\n",
        "                p_perm = (perm_count + 1) / (N_PERMS + 1)\n",
        "            else:\n",
        "                p_perm = float('nan')\n",
        "            model_out['part1']['gender'] = {\"energy_distance\": float(ed_obs), \"perm_p\": float(p_perm), \"n_m\": int(n_m), \"n_f\": int(n_f)}\n",
        "\n",
        "            skins_unique = [\"light\",\"mid\",\"dark\"]\n",
        "            skin_pairs = [(\"light\",\"mid\"), (\"light\",\"dark\"), (\"mid\",\"dark\")]\n",
        "            model_out['part1']['skin_pairwise'] = {}\n",
        "            for a,b in skin_pairs:\n",
        "                mask_a = (skins == a); mask_b = (skins == b)\n",
        "                na = int(mask_a.sum()); nb = int(mask_b.sum())\n",
        "                if na < 2 or nb < 2:\n",
        "                    model_out['part1']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"energy_distance\": None, \"perm_p\": None, \"n_a\": na, \"n_b\": nb}\n",
        "                else:\n",
        "                    ed_ab = multivariate_energy_distance(feat[mask_a], feat[mask_b])\n",
        "                    idxs = np.where((skins==a)|(skins==b))[0]\n",
        "                    na = int(mask_a.sum()); nb = int(mask_b.sum())\n",
        "                    count_ge = 0\n",
        "                    for i in range(int(N_PERMS/2)):\n",
        "                        perm = rng.permutation(idxs)\n",
        "                        a_idx = perm[:na]; b_idx = perm[na:na+nb]\n",
        "                        if multivariate_energy_distance(feat[a_idx], feat[b_idx]) >= ed_ab:\n",
        "                            count_ge += 1\n",
        "                    p_ab = (count_ge + 1) / (int(N_PERMS/2) + 1)\n",
        "                    model_out['part1']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"energy_distance\": float(ed_ab), \"perm_p\": float(p_ab), \"n_a\": na, \"n_b\": nb}\n",
        "\n",
        "    # if part2_df is None:\n",
        "    #     model_out['part2'] = {\"error\":\"part2_csv_missing\"}\n",
        "    # else:\n",
        "    #     col_adj = None\n",
        "    #     for cand in [\"adj_proj_avg\",\"adj_proj\",\"adj_proj_avg_\"+m]:\n",
        "    #         if cand in part2_df.columns:\n",
        "    #             col_adj = cand; break\n",
        "    #     gender_col = None\n",
        "    #     for cand in [\"inferred_gender\",\"inferred_gender_\"+m,\"gender_label\",\"gender\"]:\n",
        "    #         if cand in part2_df.columns:\n",
        "    #             gender_col = cand; break\n",
        "    #     skin_col = None\n",
        "    #     for cand in [\"inferred_skin\",\"inferred_skin_\"+m,\"skin_group\",\"skin_group_label\"]:\n",
        "    #         if cand in part2_df.columns:\n",
        "    #             skin_col = cand; break\n",
        "\n",
        "    #     if col_adj is None:\n",
        "    #         model_out['part2'] = {\"error\":\"adj_proj_column_missing\"}\n",
        "    #     else:\n",
        "    #         vals = pd.to_numeric(part2_df[col_adj], errors='coerce').to_numpy(dtype=float)\n",
        "    #         genders = part2_df[gender_col].astype(str).values if gender_col is not None else np.array([\"unknown\"]*len(part2_df))\n",
        "    #         skins = part2_df[skin_col].astype(str).values if skin_col is not None else np.array([\"unknown\"]*len(part2_df))\n",
        "    #         m_vals = vals[genders==\"male\"]; f_vals = vals[genders==\"female\"]\n",
        "    #         if len(m_vals[~np.isnan(m_vals)]) < 2 or len(f_vals[~np.isnan(f_vals)]) < 2:\n",
        "    #             model_out['part2']['gender'] = {\"cohens_d\": None, \"t_p\": None, \"n_m_tokens\": int(np.sum(~np.isnan(m_vals))), \"n_f_tokens\": int(np.sum(~np.isnan(f_vals)))}\n",
        "    #         else:\n",
        "    #             d = cohens_d_from_arrays(m_vals[~np.isnan(m_vals)], f_vals[~np.isnan(f_vals)])\n",
        "    #             tstat, tp = stats.ttest_ind(m_vals[~np.isnan(m_vals)], f_vals[~np.isnan(f_vals)], equal_var=False, nan_policy='omit')\n",
        "    #             model_out['part2']['gender'] = {\"cohens_d\": float(d), \"t_p\": float(tp), \"n_m_tokens\": int(np.sum(~np.isnan(m_vals))), \"n_f_tokens\": int(np.sum(~np.isnan(f_vals)))}\n",
        "    #         model_out['part2']['skin_pairwise'] = {}\n",
        "    #         for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "    #             va = vals[skins==a]; vb = vals[skins==b]\n",
        "    #             if len(va[~np.isnan(va)])<2 or len(vb[~np.isnan(vb)])<2:\n",
        "    #                 model_out['part2']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"cohens_d\": None, \"t_p\": None, \"n_a\": int(np.sum(~np.isnan(va))), \"n_b\": int(np.sum(~np.isnan(vb)))}\n",
        "    #             else:\n",
        "    #                 d = cohens_d_from_arrays(va[~np.isnan(va)], vb[~np.isnan(vb)])\n",
        "    #                 tstat, tp = stats.ttest_ind(va[~np.isnan(va)], vb[~np.isnan(vb)], equal_var=False, nan_policy='omit')\n",
        "    #                 model_out['part2']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"cohens_d\": float(d), \"t_p\": float(tp), \"n_a\": int(np.sum(~np.isnan(va))), \"n_b\": int(np.sum(~np.isnan(vb)))}\n",
        "\n",
        "    if part3_df is None:\n",
        "        model_out['part3'] = {\"error\":\"part3_csv_missing\"}\n",
        "    else:\n",
        "        col_gender_mention = None\n",
        "        for cand in [\"gender_mention\",\"gender_mention_\"+m]:\n",
        "            if cand in part3_df.columns:\n",
        "                col_gender_mention = cand; break\n",
        "        col_race_mention = None\n",
        "        for cand in [\"race_mention\",\"race_mention_\"+m]:\n",
        "            if cand in part3_df.columns:\n",
        "                col_race_mention = cand; break\n",
        "        gender_col = None\n",
        "        for cand in [\"inferred_gender\",\"gender_label\",\"inferred_gender_\"+m,\"gender\"]:\n",
        "            if cand in part3_df.columns:\n",
        "                gender_col = cand; break\n",
        "        skin_col = None\n",
        "        for cand in [\"inferred_skin\",\"skin_group_label\",\"inferred_skin_\"+m,\"skin_group\"]:\n",
        "            if cand in part3_df.columns:\n",
        "                skin_col = cand; break\n",
        "\n",
        "        if col_gender_mention is None or gender_col is None:\n",
        "            model_out['part3']['gender_tokens'] = {\"error\":\"missing_columns\"}\n",
        "        else:\n",
        "            flags = pd.to_numeric(part3_df[col_gender_mention], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = part3_df[gender_col].astype(str).values\n",
        "            xm = int(np.nansum(flags[genders==\"male\"]==1)) if (genders==\"male\").sum()>0 else 0\n",
        "            nm = int((genders==\"male\").sum())\n",
        "            xf = int(np.nansum(flags[genders==\"female\"]==1)) if (genders==\"female\").sum()>0 else 0\n",
        "            nf = int((genders==\"female\").sum())\n",
        "            z, p_two = two_prop_ztest(xm,nm,xf,nf)\n",
        "            table = np.array([[nm-xm, xm],[nf-xf, xf]])\n",
        "            try:\n",
        "                chi2, chi_p, _, _ = stats.chi2_contingency(table)\n",
        "            except Exception:\n",
        "                chi2, chi_p = None, None\n",
        "            model_out['part3']['gender_tokens'] = {\"x_m\": xm, \"n_m\": nm, \"p_m\": (xm/nm if nm>0 else None),\n",
        "                                                   \"x_f\": xf, \"n_f\": nf, \"p_f\": (xf/nf if nf>0 else None),\n",
        "                                                   \"z\": float(z) if not math.isnan(z) else None, \"p_two_sided\": float(p_two) if not math.isnan(p_two) else None,\n",
        "                                                   \"chi2\": float(chi2) if chi2 is not None else None, \"chi2_p\": float(chi_p) if chi_p is not None else None}\n",
        "        if col_race_mention is None or gender_col is None:\n",
        "            model_out['part3']['race_tokens'] = {\"error\":\"missing_columns\"}\n",
        "        else:\n",
        "            flags = pd.to_numeric(part3_df[col_race_mention], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = part3_df[gender_col].astype(str).values\n",
        "            xm = int(np.nansum(flags[genders==\"male\"]==1)) if (genders==\"male\").sum()>0 else 0\n",
        "            nm = int((genders==\"male\").sum())\n",
        "            xf = int(np.nansum(flags[genders==\"female\"]==1)) if (genders==\"female\").sum()>0 else 0\n",
        "            nf = int((genders==\"female\").sum())\n",
        "            z, p_two = two_prop_ztest(xm,nm,xf,nf)\n",
        "            table = np.array([[nm-xm, xm],[nf-xf, xf]])\n",
        "            try:\n",
        "                chi2, chi_p, _, _ = stats.chi2_contingency(table)\n",
        "            except Exception:\n",
        "                chi2, chi_p = None, None\n",
        "            model_out['part3']['race_tokens'] = {\"x_m\": xm, \"n_m\": nm, \"p_m\": (xm/nm if nm>0 else None),\n",
        "                                                 \"x_f\": xf, \"n_f\": nf, \"p_f\": (xf/nf if nf>0 else None),\n",
        "                                                 \"z\": float(z) if not math.isnan(z) else None, \"p_two_sided\": float(p_two) if not math.isnan(p_two) else None,\n",
        "                                                 \"chi2\": float(chi2) if chi2 is not None else None, \"chi2_p\": float(chi_p) if chi_p is not None else None}\n",
        "\n",
        "        model_out['part3']['skin_pairwise'] = {}\n",
        "        if col_gender_mention is not None and skin_col is not None:\n",
        "            flags = pd.to_numeric(part3_df[col_gender_mention], errors='coerce').to_numpy(dtype=float)\n",
        "            skins = part3_df[skin_col].astype(str).values\n",
        "            for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                xa = int(np.nansum(flags[skins==a]==1)) if (skins==a).sum()>0 else 0\n",
        "                na = int((skins==a).sum())\n",
        "                xb = int(np.nansum(flags[skins==b]==1)) if (skins==b).sum()>0 else 0\n",
        "                nb = int((skins==b).sum())\n",
        "                if na>0 and nb>0:\n",
        "                    z, p = two_prop_ztest(xa,na,xb,nb)\n",
        "                else:\n",
        "                    z,p = float('nan'), float('nan')\n",
        "                model_out['part3']['skin_pairwise'][f\"{a}_vs_{b}_gender_mention\"] = {\"x_a\": xa, \"n_a\": na, \"p_a\": (xa/na if na>0 else None),\n",
        "                                                                                       \"x_b\": xb, \"n_b\": nb, \"p_b\": (xb/nb if nb>0 else None),\n",
        "                                                                                       \"z\": None if math.isnan(z) else float(z), \"p_two_sided\": None if math.isnan(p) else float(p)}\n",
        "        if col_race_mention is not None and skin_col is not None:\n",
        "            flags = pd.to_numeric(part3_df[col_race_mention], errors='coerce').to_numpy(dtype=float)\n",
        "            skins = part3_df[skin_col].astype(str).values\n",
        "            for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                xa = int(np.nansum(flags[skins==a]==1)) if (skins==a).sum()>0 else 0\n",
        "                na = int((skins==a).sum())\n",
        "                xb = int(np.nansum(flags[skins==b]==1)) if (skins==b).sum()>0 else 0\n",
        "                nb = int((skins==b).sum())\n",
        "                if na>0 and nb>0:\n",
        "                    z, p = two_prop_ztest(xa,na,xb,nb)\n",
        "                else:\n",
        "                    z,p = float('nan'), float('nan')\n",
        "                model_out['part3']['skin_pairwise'][f\"{a}_vs_{b}_race_mention\"] = {\"x_a\": xa, \"n_a\": na, \"p_a\": (xa/na if na>0 else None),\n",
        "                                                                                      \"x_b\": xb, \"n_b\": nb, \"p_b\": (xb/nb if nb>0 else None),\n",
        "                                                                                      \"z\": None if math.isnan(z) else float(z), \"p_two_sided\": None if math.isnan(p) else float(p)}\n",
        "\n",
        "    if part4_df is None:\n",
        "        model_out['part4'] = {\"error\":\"part4_csv_missing\"}\n",
        "    else:\n",
        "        col_sent = None\n",
        "        for cand in [\"sentiment_compound\",\"sentiment_compound_\"+m]:\n",
        "            if cand in part4_df.columns:\n",
        "                col_sent = cand; break\n",
        "        gender_col = None\n",
        "        for cand in [\"inferred_gender\",\"gender_label\",\"inferred_gender_\"+m,\"gender\"]:\n",
        "            if cand in part4_df.columns:\n",
        "                gender_col = cand; break\n",
        "        skin_col = None\n",
        "        for cand in [\"inferred_skin\",\"skin_group_label\",\"inferred_skin_\"+m,\"skin_group\"]:\n",
        "            if cand in part4_df.columns:\n",
        "                skin_col = cand; break\n",
        "        if col_sent is None:\n",
        "            model_out['part4'] = {\"error\":\"sentiment_column_missing\"}\n",
        "        else:\n",
        "            svals = pd.to_numeric(part4_df[col_sent], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = part4_df[gender_col].astype(str).values if gender_col is not None else np.array([\"unknown\"]*len(part4_df))\n",
        "            skins = part4_df[skin_col].astype(str).values if skin_col is not None else np.array([\"unknown\"]*len(part4_df))\n",
        "            a = svals[genders==\"male\"]; b = svals[genders==\"female\"]\n",
        "            if len(a[~np.isnan(a)])>=2 and len(b[~np.isnan(b)])>=2:\n",
        "                tstat, t_p = stats.ttest_ind(a[~np.isnan(a)], b[~np.isnan(b)], equal_var=False, nan_policy='omit')\n",
        "                mw = stats.mannwhitneyu(a[~np.isnan(a)], b[~np.isnan(b)], alternative='two-sided')\n",
        "                model_out['part4']['gender'] = {\"male_mean\": float(np.nanmean(a)), \"female_mean\": float(np.nanmean(b)),\n",
        "                                                \"t_p\": float(t_p), \"t_stat\": float(tstat), \"mw_p\": float(mw.pvalue)}\n",
        "            else:\n",
        "                model_out['part4']['gender'] = {\"error\":\"insufficient_samples\"}\n",
        "            model_out['part4']['skin_pairwise'] = {}\n",
        "            for a_s,b_s in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                va = svals[skins==a_s]; vb = svals[skins==b_s]\n",
        "                if len(va[~np.isnan(va)])>=2 and len(vb[~np.isnan(vb)])>=2:\n",
        "                    tstat, t_p = stats.ttest_ind(va[~np.isnan(va)], vb[~np.isnan(vb)], equal_var=False, nan_policy='omit')\n",
        "                    mw = stats.mannwhitneyu(va[~np.isnan(va)], vb[~np.isnan(vb)], alternative='two-sided')\n",
        "                    model_out['part4']['skin_pairwise'][f\"{a_s}_vs_{b_s}\"] = {\"a_mean\": float(np.nanmean(va)), \"b_mean\": float(np.nanmean(vb)),\n",
        "                                                                              \"t_p\": float(t_p), \"t_stat\": float(tstat), \"mw_p\": float(mw.pvalue)}\n",
        "                else:\n",
        "                    model_out['part4']['skin_pairwise'][f\"{a_s}_vs_{b_s}\"] = {\"error\":\"insufficient_samples\"}\n",
        "\n",
        "    if part5_df is None:\n",
        "        model_out['part5'] = {\"error\":\"part5_csv_missing\"}\n",
        "    else:\n",
        "        col_ttr = None\n",
        "        for cand in [\"ttr\",\"ttr_\"+m]:\n",
        "            if cand in part5_df.columns:\n",
        "                col_ttr = cand; break\n",
        "        gender_col = None\n",
        "        for cand in [\"inferred_gender\",\"gender_label\",\"inferred_gender_\"+m,\"gender\"]:\n",
        "            if cand in part5_df.columns:\n",
        "                gender_col = cand; break\n",
        "        skin_col = None\n",
        "        for cand in [\"inferred_skin\",\"skin_group_label\",\"inferred_skin_\"+m,\"skin_group\"]:\n",
        "            if cand in part5_df.columns:\n",
        "                skin_col = cand; break\n",
        "        if col_ttr is None:\n",
        "            model_out['part5'] = {\"error\":\"ttr_column_missing\"}\n",
        "        else:\n",
        "            tvals = pd.to_numeric(part5_df[col_ttr], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = part5_df[gender_col].astype(str).values if gender_col is not None else np.array([\"unknown\"]*len(part5_df))\n",
        "            skins = part5_df[skin_col].astype(str).values if skin_col is not None else np.array([\"unknown\"]*len(part5_df))\n",
        "            a = tvals[genders==\"male\"]; b = tvals[genders==\"female\"]\n",
        "            if len(a[~np.isnan(a)])>=2 and len(b[~np.isnan(b)])>=2:\n",
        "                tstat, t_p = stats.ttest_ind(a[~np.isnan(a)], b[~np.isnan(b)], equal_var=False, nan_policy='omit')\n",
        "                mw = stats.mannwhitneyu(a[~np.isnan(a)], b[~np.isnan(b)], alternative='two-sided')\n",
        "                model_out['part5']['gender'] = {\"male_mean\": float(np.nanmean(a)), \"female_mean\": float(np.nanmean(b)),\n",
        "                                                \"t_p\": float(t_p), \"t_stat\": float(tstat), \"mw_p\": float(mw.pvalue)}\n",
        "            else:\n",
        "                model_out['part5']['gender'] = {\"error\":\"insufficient_samples\"}\n",
        "            model_out['part5']['skin_pairwise'] = {}\n",
        "            for a_s,b_s in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                va = tvals[skins==a_s]; vb = tvals[skins==b_s]\n",
        "                if len(va[~np.isnan(va)])>=2 and len(vb[~np.isnan(vb)])>=2:\n",
        "                    tstat, t_p = stats.ttest_ind(va[~np.isnan(va)], vb[~np.isnan(vb)], equal_var=False, nan_policy='omit')\n",
        "                    mw = stats.mannwhitneyu(va[~np.isnan(va)], vb[~np.isnan(vb)], alternative='two-sided')\n",
        "                    model_out['part5']['skin_pairwise'][f\"{a_s}_vs_{b_s}\"] = {\"a_mean\": float(np.nanmean(va)), \"b_mean\": float(np.nanmean(vb)),\n",
        "                                                                               \"t_p\": float(t_p), \"t_stat\": float(tstat), \"mw_p\": float(mw.pvalue)}\n",
        "                else:\n",
        "                    model_out['part5']['skin_pairwise'][f\"{a_s}_vs_{b_s}\"] = {\"error\":\"insufficient_samples\"}\n",
        "\n",
        "    anova_skin = {}\n",
        "    per_model_numeric = {\n",
        "        \"adj_proj_avg\": (\"part2\", [\"adj_proj_avg\",\"adj_proj\"]),\n",
        "        \"sentiment_compound\": (\"part4\", [\"sentiment_compound\"]),\n",
        "        \"ttr\": (\"part5\", [\"ttr\"]),\n",
        "        \"sim_to_male\": (\"part1\", [\"sim_to_male_centroid\",\"sim_to_male_centroid_\"+m]),\n",
        "        \"sim_to_female\": (\"part1\", [\"sim_to_female_centroid\",\"sim_to_female_centroid_\"+m]),\n",
        "        \"sim_skin_light\": (\"part1\", [\"sim_to_skin_light_centroid\",\"sim_to_skin_light_centroid_\"+m]),\n",
        "        \"sim_skin_mid\": (\"part1\", [\"sim_to_skin_mid_centroid\",\"sim_to_skin_mid_centroid_\"+m]),\n",
        "        \"sim_skin_dark\": (\"part1\", [\"sim_to_skin_dark_centroid\",\"sim_to_skin_dark_centroid_\"+m])\n",
        "    }\n",
        "\n",
        "    def find_first_column(df, candidates):\n",
        "        if df is None: return None\n",
        "        for c in candidates:\n",
        "            if isinstance(c, str):\n",
        "                if c in df.columns:\n",
        "                    return c\n",
        "        for cand in [\"adj_proj\",\"sentiment\",\"ttr\",\"sim_to_male\",\"sim_to_female\",\"sim_to_skin\"]:\n",
        "            for col in df.columns:\n",
        "                if cand in col:\n",
        "                    return col\n",
        "        return None\n",
        "\n",
        "    for metric, (part_key, cand_list) in per_model_numeric.items():\n",
        "        df = csvs[m].get(part_key)\n",
        "        if df is None:\n",
        "            anova_skin[metric] = {\"error\":\"missing_part_csv\"}\n",
        "            continue\n",
        "        col = find_first_column(df, cand_list)\n",
        "        if col is None:\n",
        "            anova_skin[metric] = {\"error\":\"metric_column_missing\"}\n",
        "            continue\n",
        "        skin_col = None\n",
        "        for cand in [\"inferred_skin\",\"skin_group_label\",\"inferred_skin_\"+m,\"skin_group\"]:\n",
        "            if cand in df.columns:\n",
        "                skin_col = cand; break\n",
        "        if skin_col is None:\n",
        "            anova_skin[metric] = {\"error\":\"skin_column_missing\"}\n",
        "            continue\n",
        "        arrs = []\n",
        "        ns = []\n",
        "        for s in [\"light\",\"mid\",\"dark\"]:\n",
        "            vals = pd.to_numeric(df.loc[df[skin_col].astype(str)==s, col], errors='coerce').to_numpy(dtype=float)\n",
        "            arrs.append(vals)\n",
        "            ns.append(int(np.sum(~np.isnan(vals))))\n",
        "        f, p = compute_anova_across_models(arrs)\n",
        "        anova_skin[metric] = {\"f\": f, \"p\": p, \"n_per_group\": ns}\n",
        "\n",
        "    model_out['anova_skin'] = anova_skin\n",
        "\n",
        "    results['models'][m] = model_out\n",
        "\n",
        "print(\"\\nComputing cross-model ANOVA and chi2 across models.\")\n",
        "\n",
        "numeric_metrics = {\n",
        "    \"adj_proj_avg\": (\"part2\", [\"adj_proj_avg\", \"adj_proj\"]),\n",
        "    \"sentiment_compound\": (\"part4\", [\"sentiment_compound\"]),\n",
        "    \"ttr\": (\"part5\", [\"ttr\"]),\n",
        "    \"sim_to_male\": (\"part1\", [\"sim_to_male_centroid\"] + [f\"sim_to_male_centroid_{mm}\" for mm in MODEL_NAMES]),\n",
        "    \"sim_to_female\": (\"part1\", [\"sim_to_female_centroid\"] + [f\"sim_to_female_centroid_{mm}\" for mm in MODEL_NAMES]),\n",
        "    \"sim_skin_light\": (\"part1\", [\"sim_to_skin_light_centroid\"] + [f\"sim_to_skin_light_centroid_{mm}\" for mm in MODEL_NAMES]),\n",
        "    \"sim_skin_mid\": (\"part1\", [\"sim_to_skin_mid_centroid\"] + [f\"sim_to_skin_mid_centroid_{mm}\" for mm in MODEL_NAMES]),\n",
        "    \"sim_skin_dark\": (\"part1\", [\"sim_to_skin_dark_centroid\"] + [f\"sim_to_skin_dark_centroid_{mm}\" for mm in MODEL_NAMES])\n",
        "}\n",
        "\n",
        "def find_first_column(df, candidates):\n",
        "    if df is None: return None\n",
        "    for c in candidates:\n",
        "        if isinstance(c, str):\n",
        "            if c in df.columns:\n",
        "                return c\n",
        "    for cand in [\"adj_proj\",\"sentiment\",\"ttr\",\"sim_to_male\",\"sim_to_female\",\"sim_to_skin\"]:\n",
        "        for col in df.columns:\n",
        "            if cand in col:\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "cross = {\"anova\":{}, \"chi2\":{}}\n",
        "gender_groups = [\"male\",\"female\"]\n",
        "skin_groups = [\"light\",\"mid\",\"dark\"]\n",
        "\n",
        "for metric, (part_key, cand_list) in numeric_metrics.items():\n",
        "    cross['anova'][metric] = {\"gender\":{}, \"skin\":{}}\n",
        "    for g in gender_groups:\n",
        "        arrs = []\n",
        "        ns = []\n",
        "        for m in MODEL_NAMES:\n",
        "            df = csvs[m].get(part_key)\n",
        "            if df is None:\n",
        "                arrs.append(np.array([])); ns.append(0); continue\n",
        "            col = find_first_column(df, cand_list)\n",
        "            if col is None:\n",
        "                arrs.append(np.array([])); ns.append(0); continue\n",
        "            subgroup_col = None\n",
        "            for cand in [\"inferred_gender\",\"gender_label\",\"gender\",\"inferred_gender_\"+m]:\n",
        "                if cand in df.columns:\n",
        "                    subgroup_col = cand; break\n",
        "            if subgroup_col is None:\n",
        "                arrs.append(np.array([])); ns.append(0); continue\n",
        "            vals = pd.to_numeric(df.loc[df[subgroup_col].astype(str)==g, col], errors='coerce').to_numpy(dtype=float)\n",
        "            arrs.append(vals)\n",
        "            ns.append(int(np.sum(~np.isnan(vals))))\n",
        "        f,p = compute_anova_across_models(arrs)\n",
        "        cross['anova'][metric]['gender'][g] = {\"f\": f, \"p\": p, \"n_per_model\": ns}\n",
        "    for s in skin_groups:\n",
        "        arrs = []\n",
        "        ns = []\n",
        "        for m in MODEL_NAMES:\n",
        "            df = csvs[m].get(part_key)\n",
        "            if df is None:\n",
        "                arrs.append(np.array([])); ns.append(0); continue\n",
        "            col = find_first_column(df, cand_list)\n",
        "            if col is None:\n",
        "                arrs.append(np.array([])); ns.append(0); continue\n",
        "            subgroup_col = None\n",
        "            for cand in [\"inferred_skin\",\"skin_group_label\",\"inferred_skin_\"+m,\"skin_group\"]:\n",
        "                if cand in df.columns:\n",
        "                    subgroup_col = cand; break\n",
        "            if subgroup_col is None:\n",
        "                arrs.append(np.array([])); ns.append(0); continue\n",
        "            vals = pd.to_numeric(df.loc[df[subgroup_col].astype(str)==s, col], errors='coerce').to_numpy(dtype=float)\n",
        "            arrs.append(vals)\n",
        "            ns.append(int(np.sum(~np.isnan(vals))))\n",
        "        f,p = compute_anova_across_models(arrs)\n",
        "        cross['anova'][metric]['skin'][s] = {\"f\": f, \"p\": p, \"n_per_model\": ns}\n",
        "\n",
        "binary_metrics = {\n",
        "    \"gender_mention\": (\"part3\", [\"gender_mention\"]),\n",
        "    \"race_mention\": (\"part3\", [\"race_mention\"])\n",
        "}\n",
        "for bin_metric, (part_key, cand_list) in binary_metrics.items():\n",
        "    cross['chi2'][bin_metric] = {\"gender\":{}, \"skin\":{}}\n",
        "    for g in gender_groups:\n",
        "        arrs = []\n",
        "        for m in MODEL_NAMES:\n",
        "            df = csvs[m].get(part_key)\n",
        "            if df is None:\n",
        "                arrs.append(np.array([])); continue\n",
        "            col = find_first_column(df, cand_list)\n",
        "            if col is None:\n",
        "                arrs.append(np.array([])); continue\n",
        "            subgroup_col = None\n",
        "            for cand in [\"inferred_gender\",\"gender_label\",\"gender\"]:\n",
        "                if cand in df.columns:\n",
        "                    subgroup_col = cand; break\n",
        "            if subgroup_col is None:\n",
        "                arrs.append(np.array([])); continue\n",
        "            vals = pd.to_numeric(df.loc[df[subgroup_col].astype(str)==g, col], errors='coerce').to_numpy(dtype=float)\n",
        "            arrs.append(vals)\n",
        "        chi2, chi_p, table = compute_chi2_across_models_binary(arrs)\n",
        "        counts = []\n",
        "        for a in arrs:\n",
        "            a = np.asarray(a); a = a[~np.isnan(a)]\n",
        "            counts.append({\"n\": int(len(a)), \"x\": int(np.sum(a==1)) if len(a)>0 else 0})\n",
        "        cross['chi2'][bin_metric]['gender'][g] = {\"chi2\": chi2, \"chi2_p\": chi_p, \"counts_per_model\": counts, \"contingency_table\": table}\n",
        "    for s in skin_groups:\n",
        "        arrs = []\n",
        "        for m in MODEL_NAMES:\n",
        "            df = csvs[m].get(part_key)\n",
        "            if df is None:\n",
        "                arrs.append(np.array([])); continue\n",
        "            col = find_first_column(df, cand_list)\n",
        "            if col is None:\n",
        "                arrs.append(np.array([])); continue\n",
        "            subgroup_col = None\n",
        "            for cand in [\"inferred_skin\",\"skin_group_label\",\"inferred_skin\",\"skin_group\"]:\n",
        "                if cand in df.columns:\n",
        "                    subgroup_col = cand; break\n",
        "            if subgroup_col is None:\n",
        "                arrs.append(np.array([])); continue\n",
        "            vals = pd.to_numeric(df.loc[df[subgroup_col].astype(str)==s, col], errors='coerce').to_numpy(dtype=float)\n",
        "            arrs.append(vals)\n",
        "        chi2, chi_p, table = compute_chi2_across_models_binary(arrs)\n",
        "        counts = []\n",
        "        for a in arrs:\n",
        "            a = np.asarray(a); a = a[~np.isnan(a)]\n",
        "            counts.append({\"n\": int(len(a)), \"x\": int(np.sum(a==1)) if len(a)>0 else 0})\n",
        "        cross['chi2'][bin_metric]['skin'][s] = {\"chi2\": chi2, \"chi2_p\": chi_p, \"counts_per_model\": counts, \"contingency_table\": table}\n",
        "\n",
        "final_output = {\"timestamp\": datetime.now(timezone.utc).isoformat(), \"models\": results['models'], \"cross_model\": cross}\n",
        "out_json_path = os.path.join(OUT_DIR, \"part7_recomputed_with_anova.json\")\n",
        "with open(out_json_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump(final_output, jf, indent=2)\n",
        "print(\"\\nWrote final JSON:\", out_json_path)\n",
        "\n",
        "rows = []\n",
        "for m in MODEL_NAMES:\n",
        "    md = results['models'].get(m, {})\n",
        "    p1g = md.get('part1', {}).get('gender', {})\n",
        "    rows.append({\"model\": m, \"metric\":\"part1_energy_distance_gender\", \"value\": p1g.get('energy_distance'), \"p\": p1g.get('perm_p')})\n",
        "    spp = md.get('part1', {}).get('skin_pairwise', {})\n",
        "    rows.append({\"model\": m, \"metric\":\"part1_energy_distance_light_vs_dark\", \"value\": spp.get('light_vs_dark',{}).get('energy_distance'), \"p\": spp.get('light_vs_dark',{}).get('perm_p')})\n",
        "    p2g = md.get('part2', {}).get('gender', {})\n",
        "    rows.append({\"model\": m, \"metric\":\"part2_adj_cohens_d\", \"value\": p2g.get('cohens_d'), \"p\": p2g.get('t_p')})\n",
        "    p3g = md.get('part3', {}).get('gender_tokens', {})\n",
        "    rows.append({\"model\": m, \"metric\":\"part3_gender_token_rate_diff\", \"value\": (p3g.get('p_m') - p3g.get('p_f')) if p3g.get('p_m') is not None else None, \"p\": p3g.get('chi2_p')})\n",
        "    p3r = md.get('part3', {}).get('race_tokens', {})\n",
        "    rows.append({\"model\": m, \"metric\":\"part3_race_token_rate_diff\", \"value\": (p3r.get('p_m') - p3r.get('p_f')) if p3r.get('p_m') is not None else None, \"p\": p3r.get('chi2_p')})\n",
        "    p4g = md.get('part4', {}).get('gender', {})\n",
        "    rows.append({\"model\": m, \"metric\":\"part4_sentiment_mean_diff\", \"value\": (p4g.get('female_mean') - p4g.get('male_mean')) if 'female_mean' in p4g else None, \"p\": p4g.get('t_p')})\n",
        "    p5g = md.get('part5', {}).get('gender', {})\n",
        "    rows.append({\"model\": m, \"metric\":\"part5_ttr_mean_diff\", \"value\": (p5g.get('female_mean') - p5g.get('male_mean')) if 'female_mean' in p5g else None, \"p\": p5g.get('t_p')})\n",
        "\n",
        "table_df = pd.DataFrame(rows)\n",
        "csv_out = os.path.join(OUT_DIR, \"part7_recomputed_table_for_paper.csv\")\n",
        "table_df.to_csv(csv_out, index=False)\n",
        "print(\"Wrote compact CSV for paper:\", csv_out)\n",
        "\n",
        "print(\"\\nDone. Outputs written to:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import cdist\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/image_captioning_bias\"\n",
        "IN_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs\")\n",
        "OUT_DIR = os.path.join(DRIVE_BASE, \"bias_analysis_outputs_part7_pvalues\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAMES = [\"chatgpt\",\"claude\",\"gemini\"]\n",
        "N_PERMS = 1000\n",
        "RNG_SEED = 42\n",
        "\n",
        "PART1_CSV = os.path.join(IN_DIR, \"part1_bias_metrics_{m}.csv\")\n",
        "PART2_CSV = os.path.join(IN_DIR, \"part2_bias_metrics_{m}.csv\")\n",
        "PART3_CSV = os.path.join(IN_DIR, \"part3_bias_metrics_{m}.csv\")\n",
        "PART4_CSV = os.path.join(IN_DIR, \"part4_bias_metrics_{m}.csv\")\n",
        "PART5_CSV = os.path.join(IN_DIR, \"part5_bias_metrics_{m}.csv\")\n",
        "\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not read {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def multivariate_energy_distance(X, Y):\n",
        "    X = np.asarray(X); Y = np.asarray(Y)\n",
        "    if X.size==0 or Y.size==0:\n",
        "        return float('nan')\n",
        "    if X.ndim==1: X = X.reshape(-1,1)\n",
        "    if Y.ndim==1: Y = Y.reshape(-1,1)\n",
        "    n = X.shape[0]; m = Y.shape[0]\n",
        "    cross = cdist(X, Y, metric=\"euclidean\")\n",
        "    a = (2.0/(n*m))*np.sum(cross)\n",
        "    xx = cdist(X, X, metric=\"euclidean\") if n>1 else np.zeros((1,1))\n",
        "    yy = cdist(Y, Y, metric=\"euclidean\") if m>1 else np.zeros((1,1))\n",
        "    b = (1.0/(n*n))*np.sum(xx) if n>1 else 0.0\n",
        "    c = (1.0/(m*m))*np.sum(yy) if m>1 else 0.0\n",
        "    return float(max(a - b - c, 0.0))\n",
        "\n",
        "def two_prop_ztest(x1,n1,x2,n2):\n",
        "    if n1==0 or n2==0:\n",
        "        return float('nan'), 1.0\n",
        "    p1 = x1 / n1\n",
        "    p2 = x2 / n2\n",
        "    p_pool = (x1 + x2) / (n1 + n2)\n",
        "    denom = math.sqrt(max(p_pool*(1-p_pool)*(1.0/n1 + 1.0/n2), 1e-20))\n",
        "    z = (p1 - p2) / denom\n",
        "    p = 2.0 * (1.0 - stats.norm.cdf(abs(z)))\n",
        "    return float(z), float(p)\n",
        "\n",
        "def cohens_d_from_arrays(a,b):\n",
        "    a = np.asarray(a); b = np.asarray(b)\n",
        "    a = a[~np.isnan(a)]; b = b[~np.isnan(b)]\n",
        "    if len(a)<2 or len(b)<2:\n",
        "        return float('nan')\n",
        "    ma, mb = np.mean(a), np.mean(b)\n",
        "    sa, sb = np.std(a, ddof=1), np.std(b, ddof=1)\n",
        "    pooled = math.sqrt(((len(a)-1)*sa*sa + (len(b)-1)*sb*sb) / (len(a)+len(b)-2))\n",
        "    if pooled == 0:\n",
        "        return float('nan')\n",
        "    return float((ma-mb)/pooled)\n",
        "\n",
        "def ensure_p(p):\n",
        "    try:\n",
        "        if p is None or (isinstance(p,float) and math.isnan(p)):\n",
        "            return 1.0\n",
        "        return float(p)\n",
        "    except:\n",
        "        return 1.0\n",
        "\n",
        "csvs = {}\n",
        "for m in MODEL_NAMES:\n",
        "    csvs[m] = {\n",
        "        \"part1\": safe_read_csv(PART1_CSV.format(m=m)),\n",
        "        \"part2\": safe_read_csv(PART2_CSV.format(m=m)),\n",
        "        \"part3\": safe_read_csv(PART3_CSV.format(m=m)),\n",
        "        \"part4\": safe_read_csv(PART4_CSV.format(m=m)),\n",
        "        \"part5\": safe_read_csv(PART5_CSV.format(m=m)),\n",
        "    }\n",
        "    sizes = {k:(len(v) if v is not None else 0) for k,v in csvs[m].items()}\n",
        "    print(f\"{m}: rows per part ->\", sizes)\n",
        "\n",
        "out = {\"timestamp\": datetime.now(timezone.utc).isoformat(), \"models\": {}}\n",
        "\n",
        "for m in MODEL_NAMES:\n",
        "    print(f\"\\nProcessing model: {m}\")\n",
        "    res = {\"part1\":{}, \"part3\":{}, \"part4\":{}, \"part5\":{}}\n",
        "    df1 = csvs[m][\"part1\"]\n",
        "    if df1 is None:\n",
        "        print(f\"  WARN part1 CSV missing for {m}; setting p-values to 1.0\")\n",
        "        res['part1'] = {\n",
        "            \"gender_perm_p\": 1.0,\n",
        "            \"skin_pairwise_perm_p\": {\"light_vs_mid\":1.0,\"light_vs_dark\":1.0,\"mid_vs_dark\":1.0}\n",
        "        }\n",
        "    else:\n",
        "        def find_col(df, candidates):\n",
        "            for c in candidates:\n",
        "                if c in df.columns: return c\n",
        "            for col in df.columns:\n",
        "                for cand in candidates:\n",
        "                    if isinstance(cand,str) and cand in col:\n",
        "                        return col\n",
        "            return None\n",
        "        c_m = find_col(df1, [\"sim_to_male_centroid\", f\"sim_to_male_centroid_{m}\"])\n",
        "        c_f = find_col(df1, [\"sim_to_female_centroid\", f\"sim_to_female_centroid_{m}\"])\n",
        "        c_light = find_col(df1, [\"sim_to_skin_light_centroid\", f\"sim_to_skin_light_centroid_{m}\"])\n",
        "        c_mid   = find_col(df1, [\"sim_to_skin_mid_centroid\", f\"sim_to_skin_mid_centroid_{m}\"])\n",
        "        c_dark  = find_col(df1, [\"sim_to_skin_dark_centroid\", f\"sim_to_skin_dark_centroid_{m}\"])\n",
        "        feature_cols = [c for c in [c_m, c_f, c_light, c_mid, c_dark] if c is not None]\n",
        "        feat = df1[feature_cols].apply(pd.to_numeric, errors='coerce').to_numpy(dtype=float) if feature_cols else np.empty((len(df1),0))\n",
        "        gender_col = next((c for c in [\"gender_label\",\"inferred_gender\",\"inferred_gender_\"+m] if c in df1.columns), None)\n",
        "        skin_col = next((c for c in [\"skin_group_label\",\"inferred_skin\",\"inferred_skin_\"+m,\"skin_group\"] if c in df1.columns), None)\n",
        "        genders = df1[gender_col].astype(str).values if gender_col is not None else np.array([\"unknown\"]*len(df1))\n",
        "        skins = df1[skin_col].astype(str).values if skin_col is not None else np.array([\"unknown\"]*len(df1))\n",
        "\n",
        "        mask_m = (genders == \"male\"); mask_f = (genders == \"female\")\n",
        "        n_m = int(mask_m.sum()); n_f = int(mask_f.sum())\n",
        "        if n_m>0 and n_f>0 and feat.size>0:\n",
        "            X = feat[mask_m]; Y = feat[mask_f]\n",
        "            ed_obs = multivariate_energy_distance(X,Y)\n",
        "            perm_ge = 0\n",
        "            idxs = np.arange(len(feat))\n",
        "            for i in range(N_PERMS):\n",
        "                perm = rng.permutation(idxs)\n",
        "                a_idx = perm[:n_m]; b_idx = perm[n_m:n_m+n_f]\n",
        "                val = multivariate_energy_distance(feat[a_idx], feat[b_idx])\n",
        "                if val >= ed_obs:\n",
        "                    perm_ge += 1\n",
        "            p_perm = (perm_ge + 1) / (N_PERMS + 1)\n",
        "        else:\n",
        "            ed_obs = float('nan'); p_perm = 1.0\n",
        "            if n_m==0 or n_f==0:\n",
        "                print(f\"  WARN part1 gender groups insufficient for {m}: n_m={n_m}, n_f={n_f}\")\n",
        "        res['part1']['gender_energy_distance'] = float(ed_obs) if not math.isnan(ed_obs) else None\n",
        "        res['part1']['gender_perm_p'] = ensure_p(p_perm)\n",
        "        res['part1']['skin_pairwise_perm_p'] = {}\n",
        "        for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "            ma = (skins==a); mb = (skins==b)\n",
        "            na = int(ma.sum()); nb = int(mb.sum())\n",
        "            if na>1 and nb>1 and feat.size>0:\n",
        "                ed_ab = multivariate_energy_distance(feat[ma], feat[mb])\n",
        "                idxs = np.where((skins==a)|(skins==b))[0]\n",
        "                count_ge = 0\n",
        "                nperm = max(200, int(N_PERMS/5))\n",
        "                for i in range(nperm):\n",
        "                    perm = rng.permutation(idxs)\n",
        "                    a_idx = perm[:na]; b_idx = perm[na:na+nb]\n",
        "                    if multivariate_energy_distance(feat[a_idx], feat[b_idx]) >= ed_ab:\n",
        "                        count_ge += 1\n",
        "                p_ab = (count_ge + 1) / (nperm + 1)\n",
        "            else:\n",
        "                ed_ab = float('nan'); p_ab = 1.0\n",
        "                if na<=1 or nb<=1:\n",
        "                    print(f\"  WARN part1 skin pair insufficient {a} vs {b} for {m}: na={na}, nb={nb}\")\n",
        "            res['part1']['skin_pairwise_perm_p'][f\"{a}_vs_{b}\"] = ensure_p(p_ab)\n",
        "\n",
        "    df3 = csvs[m][\"part3\"]\n",
        "    if df3 is None:\n",
        "        print(f\"  WARN part3 CSV missing for {m}; setting p-values to 1.0\")\n",
        "        res['part3']['gender_tokens_chi2_p'] = 1.0\n",
        "        res['part3']['race_tokens_chi2_p'] = 1.0\n",
        "        res['part3']['skin_pairwise_gender_mention_p'] = {\"light_vs_mid\":1.0,\"light_vs_dark\":1.0,\"mid_vs_dark\":1.0}\n",
        "        res['part3']['skin_pairwise_race_mention_p'] = {\"light_vs_mid\":1.0,\"light_vs_dark\":1.0,\"mid_vs_dark\":1.0}\n",
        "    else:\n",
        "        col_gender_mention = next((c for c in [\"gender_mention\", f\"gender_mention_{m}\"] if c in df3.columns), None)\n",
        "        col_race_mention = next((c for c in [\"race_mention\", f\"race_mention_{m}\"] if c in df3.columns), None)\n",
        "        gender_col = next((c for c in [\"inferred_gender\",\"gender_label\", f\"inferred_gender_{m}\"] if c in df3.columns), None)\n",
        "        skin_col = next((c for c in [\"inferred_skin\",\"skin_group_label\", f\"inferred_skin_{m}\", \"skin_group\"] if c in df3.columns), None)\n",
        "\n",
        "        if col_gender_mention and gender_col:\n",
        "            flags = pd.to_numeric(df3[col_gender_mention], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = df3[gender_col].astype(str).values\n",
        "            xm = int(np.nansum(flags[genders==\"male\"]==1)) if (genders==\"male\").sum()>0 else 0\n",
        "            nm = int((genders==\"male\").sum())\n",
        "            xf = int(np.nansum(flags[genders==\"female\"]==1)) if (genders==\"female\").sum()>0 else 0\n",
        "            nf = int((genders==\"female\").sum())\n",
        "            try:\n",
        "                table = np.array([[nm-xm, xm],[nf-xf, xf]])\n",
        "                chi2, chi_p, _, _ = stats.chi2_contingency(table)\n",
        "            except Exception:\n",
        "                chi2, chi_p = None, None\n",
        "            res['part3']['gender_tokens_chi2_p'] = ensure_p(chi_p)\n",
        "        else:\n",
        "            res['part3']['gender_tokens_chi2_p'] = 1.0\n",
        "\n",
        "        if col_race_mention and gender_col:\n",
        "            flags = pd.to_numeric(df3[col_race_mention], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = df3[gender_col].astype(str).values\n",
        "            xm = int(np.nansum(flags[genders==\"male\"]==1)) if (genders==\"male\").sum()>0 else 0\n",
        "            nm = int((genders==\"male\").sum())\n",
        "            xf = int(np.nansum(flags[genders==\"female\"]==1)) if (genders==\"female\").sum()>0 else 0\n",
        "            nf = int((genders==\"female\").sum())\n",
        "            try:\n",
        "                table = np.array([[nm-xm, xm],[nf-xf, xf]])\n",
        "                chi2, chi_p, _, _ = stats.chi2_contingency(table)\n",
        "            except Exception:\n",
        "                chi2, chi_p = None, None\n",
        "            res['part3']['race_tokens_chi2_p'] = ensure_p(chi_p)\n",
        "        else:\n",
        "            res['part3']['race_tokens_chi2_p'] = 1.0\n",
        "\n",
        "        res['part3']['skin_pairwise_gender_mention_p'] = {}\n",
        "        res['part3']['skin_pairwise_race_mention_p'] = {}\n",
        "        if skin_col:\n",
        "            if col_gender_mention:\n",
        "                flags = pd.to_numeric(df3[col_gender_mention], errors='coerce').to_numpy(dtype=float)\n",
        "                skins = df3[skin_col].astype(str).values\n",
        "                for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                    xa = int(np.nansum(flags[skins==a]==1)) if (skins==a).sum()>0 else 0\n",
        "                    na = int((skins==a).sum())\n",
        "                    xb = int(np.nansum(flags[skins==b]==1)) if (skins==b).sum()>0 else 0\n",
        "                    nb = int((skins==b).sum())\n",
        "                    z,p = two_prop_ztest(xa,na,xb,nb)\n",
        "                    res['part3']['skin_pairwise_gender_mention_p'][f\"{a}_vs_{b}\"] = ensure_p(p)\n",
        "            else:\n",
        "                for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                    res['part3']['skin_pairwise_gender_mention_p'][f\"{a}_vs_{b}\"] = 1.0\n",
        "\n",
        "            if col_race_mention:\n",
        "                flags = pd.to_numeric(df3[col_race_mention], errors='coerce').to_numpy(dtype=float)\n",
        "                skins = df3[skin_col].astype(str).values\n",
        "                for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                    xa = int(np.nansum(flags[skins==a]==1)) if (skins==a).sum()>0 else 0\n",
        "                    na = int((skins==a).sum())\n",
        "                    xb = int(np.nansum(flags[skins==b]==1)) if (skins==b).sum()>0 else 0\n",
        "                    nb = int((skins==b).sum())\n",
        "                    z,p = two_prop_ztest(xa,na,xb,nb)\n",
        "                    res['part3']['skin_pairwise_race_mention_p'][f\"{a}_vs_{b}\"] = ensure_p(p)\n",
        "            else:\n",
        "                for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                    res['part3']['skin_pairwise_race_mention_p'][f\"{a}_vs_{b}\"] = 1.0\n",
        "        else:\n",
        "            for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                res['part3']['skin_pairwise_gender_mention_p'][f\"{a}_vs_{b}\"] = 1.0\n",
        "                res['part3']['skin_pairwise_race_mention_p'][f\"{a}_vs_{b}\"] = 1.0\n",
        "\n",
        "    df4 = csvs[m][\"part4\"]\n",
        "    res['part4']['gender_t_p'] = 1.0\n",
        "    res['part4']['gender_mw_p'] = 1.0\n",
        "    res['part4']['skin_pairwise'] = {\"light_vs_mid\":{\"t_p\":1.0,\"mw_p\":1.0}, \"light_vs_dark\":{\"t_p\":1.0,\"mw_p\":1.0},\"mid_vs_dark\":{\"t_p\":1.0,\"mw_p\":1.0}}\n",
        "    if df4 is not None:\n",
        "        col_sent = next((c for c in [\"sentiment_compound\", f\"sentiment_compound_{m}\"] if c in df4.columns), None)\n",
        "        gender_col = next((c for c in [\"inferred_gender\",\"gender_label\", f\"inferred_gender_{m}\"] if c in df4.columns), None)\n",
        "        skin_col = next((c for c in [\"inferred_skin\",\"skin_group_label\", f\"inferred_skin_{m}\", \"skin_group\"] if c in df4.columns), None)\n",
        "        if col_sent and gender_col:\n",
        "            svals = pd.to_numeric(df4[col_sent], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = df4[gender_col].astype(str).values\n",
        "            a = svals[genders==\"male\"]; b = svals[genders==\"female\"]\n",
        "            if np.sum(~np.isnan(a))>=2 and np.sum(~np.isnan(b))>=2:\n",
        "                tstat, t_p = stats.ttest_ind(a[~np.isnan(a)], b[~np.isnan(b)], equal_var=False, nan_policy='omit')\n",
        "                mw = stats.mannwhitneyu(a[~np.isnan(a)], b[~np.isnan(b)], alternative='two-sided')\n",
        "                res['part4']['gender_t_p'] = ensure_p(t_p)\n",
        "                res['part4']['gender_mw_p'] = ensure_p(mw.pvalue)\n",
        "        if col_sent and skin_col:\n",
        "            svals = pd.to_numeric(df4[col_sent], errors='coerce').to_numpy(dtype=float)\n",
        "            skins = df4[skin_col].astype(str).values\n",
        "            for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                va = svals[skins==a]; vb = svals[skins==b]\n",
        "                if np.sum(~np.isnan(va))>=2 and np.sum(~np.isnan(vb))>=2:\n",
        "                    tstat, t_p = stats.ttest_ind(va[~np.isnan(va)], vb[~np.isnan(vb)], equal_var=False, nan_policy='omit')\n",
        "                    mw = stats.mannwhitneyu(va[~np.isnan(va)], vb[~np.isnan(vb)], alternative='two-sided')\n",
        "                    res['part4']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"t_p\": ensure_p(t_p), \"mw_p\": ensure_p(mw.pvalue)}\n",
        "                else:\n",
        "                    res['part4']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"t_p\": 1.0, \"mw_p\": 1.0}\n",
        "\n",
        "    df5 = csvs[m][\"part5\"]\n",
        "    res['part5']['gender_t_p'] = 1.0\n",
        "    res['part5']['gender_mw_p'] = 1.0\n",
        "    res['part5']['skin_pairwise'] = {\"light_vs_mid\":{\"t_p\":1.0,\"mw_p\":1.0}, \"light_vs_dark\":{\"t_p\":1.0,\"mw_p\":1.0},\"mid_vs_dark\":{\"t_p\":1.0,\"mw_p\":1.0}}\n",
        "    if df5 is not None:\n",
        "        col_ttr = next((c for c in [\"ttr\", f\"ttr_{m}\"] if c in df5.columns), None)\n",
        "        gender_col = next((c for c in [\"inferred_gender\",\"gender_label\", f\"inferred_gender_{m}\"] if c in df5.columns), None)\n",
        "        skin_col = next((c for c in [\"inferred_skin\",\"skin_group_label\", f\"inferred_skin_{m}\", \"skin_group\"] if c in df5.columns), None)\n",
        "        if col_ttr and gender_col:\n",
        "            tvals = pd.to_numeric(df5[col_ttr], errors='coerce').to_numpy(dtype=float)\n",
        "            genders = df5[gender_col].astype(str).values\n",
        "            a = tvals[genders==\"male\"]; b = tvals[genders==\"female\"]\n",
        "            if np.sum(~np.isnan(a))>=2 and np.sum(~np.isnan(b))>=2:\n",
        "                tstat, t_p = stats.ttest_ind(a[~np.isnan(a)], b[~np.isnan(b)], equal_var=False, nan_policy='omit')\n",
        "                mw = stats.mannwhitneyu(a[~np.isnan(a)], b[~np.isnan(b)], alternative='two-sided')\n",
        "                res['part5']['gender_t_p'] = ensure_p(t_p)\n",
        "                res['part5']['gender_mw_p'] = ensure_p(mw.pvalue)\n",
        "        if col_ttr and skin_col:\n",
        "            tvals = pd.to_numeric(df5[col_ttr], errors='coerce').to_numpy(dtype=float)\n",
        "            skins = df5[skin_col].astype(str).values\n",
        "            for a,b in [(\"light\",\"mid\"),(\"light\",\"dark\"),(\"mid\",\"dark\")]:\n",
        "                va = tvals[skins==a]; vb = tvals[skins==b]\n",
        "                if np.sum(~np.isnan(va))>=2 and np.sum(~np.isnan(vb))>=2:\n",
        "                    tstat, t_p = stats.ttest_ind(va[~np.isnan(va)], vb[~np.isnan(vb)], equal_var=False, nan_policy='omit')\n",
        "                    mw = stats.mannwhitneyu(va[~np.isnan(va)], vb[~np.isnan(vb)], alternative='two-sided')\n",
        "                    res['part5']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"t_p\": ensure_p(t_p), \"mw_p\": ensure_p(mw.pvalue)}\n",
        "                else:\n",
        "                    res['part5']['skin_pairwise'][f\"{a}_vs_{b}\"] = {\"t_p\": 1.0, \"mw_p\": 1.0}\n",
        "\n",
        "    out['models'][m] = res\n",
        "\n",
        "out_path = os.path.join(OUT_DIR, \"part7_pvalues_filled.json\")\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump(out, jf, indent=2)\n",
        "print(\"\\nWrote JSON:\", out_path)\n",
        "\n",
        "rows = []\n",
        "for m in MODEL_NAMES:\n",
        "    md = out['models'][m]\n",
        "    rows.append({\"model\":m, \"metric\":\"part1_gender_perm_p\", \"M_vs_F_p\": md['part1']['gender_perm_p'] if 'gender_perm_p' in md['part1'] else md['part1'].get('gender_perm_p',1.0)})\n",
        "    spp = md['part1'].get('skin_pairwise_perm_p', {})\n",
        "    for pair in [\"light_vs_mid\",\"mid_vs_dark\",\"light_vs_dark\"]:\n",
        "        rows.append({\"model\":m, \"metric\":f\"part1_{pair}_perm_p\", \"p\": spp.get(pair,1.0)})\n",
        "\n",
        "    rows.append({\"model\":m, \"metric\":\"part3_gender_tokens_chi2_p\", \"p\": md['part3'].get('gender_tokens_chi2_p',1.0)})\n",
        "    rows.append({\"model\":m, \"metric\":\"part3_race_tokens_chi2_p\", \"p\": md['part3'].get('race_tokens_chi2_p',1.0)})\n",
        "    for pair in [\"light_vs_mid\",\"mid_vs_dark\",\"light_vs_dark\"]:\n",
        "        rows.append({\"model\":m, \"metric\":f\"part3_skin_gender_mention_{pair}_p\", \"p\": md['part3']['skin_pairwise_gender_mention_p'].get(pair,1.0)})\n",
        "        rows.append({\"model\":m, \"metric\":f\"part3_skin_race_mention_{pair}_p\", \"p\": md['part3']['skin_pairwise_race_mention_p'].get(pair,1.0)})\n",
        "\n",
        "    rows.append({\"model\":m, \"metric\":\"part4_gender_t_p\", \"p\": md['part4'].get('gender_t_p',1.0)})\n",
        "    rows.append({\"model\":m, \"metric\":\"part4_gender_mw_p\", \"p\": md['part4'].get('gender_mw_p',1.0)})\n",
        "    for pair in [\"light_vs_mid\",\"mid_vs_dark\",\"light_vs_dark\"]:\n",
        "        pvals = md['part4']['skin_pairwise'].get(pair, {\"t_p\":1.0,\"mw_p\":1.0})\n",
        "        rows.append({\"model\":m, \"metric\":f\"part4_{pair}_t_p\", \"p\": pvals.get(\"t_p\",1.0)})\n",
        "        rows.append({\"model\":m, \"metric\":f\"part4_{pair}_mw_p\", \"p\": pvals.get(\"mw_p\",1.0)})\n",
        "\n",
        "    rows.append({\"model\":m, \"metric\":\"part5_gender_t_p\", \"p\": md['part5'].get('gender_t_p',1.0)})\n",
        "    rows.append({\"model\":m, \"metric\":\"part5_gender_mw_p\", \"p\": md['part5'].get('gender_mw_p',1.0)})\n",
        "    for pair in [\"light_vs_mid\",\"mid_vs_dark\",\"light_vs_dark\"]:\n",
        "        pvals = md['part5']['skin_pairwise'].get(pair, {\"t_p\":1.0,\"mw_p\":1.0})\n",
        "        rows.append({\"model\":m, \"metric\":f\"part5_{pair}_t_p\", \"p\": pvals.get(\"t_p\",1.0)})\n",
        "        rows.append({\"model\":m, \"metric\":f\"part5_{pair}_mw_p\", \"p\": pvals.get(\"mw_p\",1.0)})\n",
        "\n",
        "df_rows = pd.DataFrame(rows)\n",
        "csv_out = os.path.join(OUT_DIR, \"part7_pvalues_grid.csv\")\n",
        "df_rows.to_csv(csv_out, index=False)\n",
        "print(\"Wrote CSV:\", csv_out)\n",
        "\n",
        "print(\"\\nDone. All requested p-values are present (non-null). If some tests lacked data, p=1.0 was used and a WARN was printed above.\")\n"
      ],
      "metadata": {
        "id": "DntkmoZEVNEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZAHc8bxY7rz2",
        "hMfTZDbts2DP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}